{"cells":[{"cell_type":"markdown","metadata":{"id":"tT2VMZJXYhTw"},"source":["1. Version  : 1\n","2. Time     : 5th May\n","3. Author   : Hui Zhang\n","4. Function : PBCD method to solve 2 hidden layer 0/1 DNN on MNIST\n","5. Structure: 2 hidden layer with 1000 nodes in each layer\n","6. Relation paper: PBCD method for 0/1 DNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1701,"status":"ok","timestamp":1651851626079,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"},"user_tz":-60},"id":"5L1Ah48Ll3Nu","outputId":"938bc88d-fc0c-4765-8655-bd22aba6ceb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# drive.mount(\"/content/drive/\", force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"CXfp0lN9YW3v"},"source":["# **Define relavant function and load data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh7W8cowv1-6"},"outputs":[],"source":["import numpy as np   # science calculate  instill numpy\n","import struct  # y???????\n","from sklearn.utils import shuffle  # instill scikit-learn/ scikit\n","import time  \n","import os  # o??????????\n","import scipy\n","import torch as tc\n","import tensorflow as tf\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm, trange\n","import tensorflow.keras as keras\n","from tensorflow.keras import datasets\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense,Conv2D,Dropout,Flatten,Activation,BatchNormalization,AveragePooling2D,MaxPooling2D\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","# Standardized data in 0-1\n","def normalize_data(ima):\n","    a_max = np.max(ima)  # 图像矩阵的最大值\n","    a_min = np.min(ima)  # 图像矩阵的最小值\n","    for i in range(ima.shape[0]):  # 矩阵第二维的长度：如3*4矩阵这个就是3\n","        for j in range(ima.shape[1]):  # numpy.core.fromnumeric 中的函数 第一维的长度\n","            ima[i][j] = (ima[i][j] - a_min) / (a_max - a_min)\n","    return ima  # 返回是一个矩阵\n","\n","# predeal dataset\n","def create_model(directory):\n","    # chanDim = -1\n","    input_shape = (28, 28, 1)\n","    model_original = load_model(os.path.join(directory, \"prenetwork/99.4_CNN.h5\"))\n","    \n","    model = Sequential()\n","    # Layer 1 Conv2D\n","    cnn_layer = Conv2D(filters = 32, kernel_size = 5, strides = 1, activation = 'relu', input_shape = input_shape)\n","    cnn_layer.trainable=False\n","    model.add(cnn_layer)\n","    cnn_layer2= Conv2D(filters = 32, kernel_size = 5, strides = 1, use_bias=False)\n","    cnn_layer2.trainable=False\n","    model.add(cnn_layer2)\n","    cnn_layer3=BatchNormalization()\n","    cnn_layer3.trainable=False\n","    model.add(cnn_layer3)\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n","    model.add(Dropout(0.25,name = 'representation')) ############## 需要这一层的输出\n","    # Layer 3 Conv2D\n","    cnn_layer4= Conv2D(filters = 64, kernel_size = 3, strides = 1, activation = 'relu')\n","    cnn_layer4.trainable=False\n","    model.add(cnn_layer4)\n","    cnn_layer5= Conv2D(filters = 64, kernel_size = 3, strides = 1, use_bias=False)\n","    cnn_layer5.trainable=False\n","    model.add(cnn_layer5)\n","    cnn_layer6= BatchNormalization()\n","    cnn_layer6.trainable=False\n","    model.add(cnn_layer6)\n","    # Layer 4 Pooling Layer\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n","    model.add(Dropout(0.25))\n","    model.add(Flatten(name='flatten'))\n","\n","\n","    model.add(Dense(500, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dense(300, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dense(100, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.25))\n","    model.add(Dense(20, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","\n","    model.add(Dense(10, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","    model.layers[0].set_weights(model_original.layers[0].get_weights())\n","    model.layers[1].set_weights(model_original.layers[1].get_weights())\n","    model.layers[2].set_weights(model_original.layers[2].get_weights())\n","    model.layers[6].set_weights(model_original.layers[6].get_weights())\n","    model.layers[7].set_weights(model_original.layers[7].get_weights())\n","    model.layers[8].set_weights(model_original.layers[8].get_weights())\n","    return model\n","\n","\n","# 初始化各个参数 n_x x的维数 n_h1：第一隐层节点数（靠近x的）, n_h2：第二隐层节点数, n_y： 输出y的维数\n","def initialize_with_zeros(n_x, n_h1, n_h2, n_y):\n","    tf.random.set_seed(21)\n","    W1 = tf.random.uniform([n_x, n_h1], minval = -np.sqrt(6) / np.sqrt(n_x + n_h1), maxval = np.sqrt(6) / np.sqrt(n_h1 + n_x), dtype=tf.dtypes.float32, seed=None, name=None)\n","    W2 = tf.random.uniform([n_h1, n_h2], minval = -np.sqrt(6) / np.sqrt(n_h1 + n_h2), maxval = np.sqrt(6) / np.sqrt(n_h1 + n_h2), dtype=tf.dtypes.float32, seed=None, name=None)\n","    W3 = tf.random.uniform([n_h2, n_y], minval = -np.sqrt(6) / np.sqrt(n_y + n_h2), maxval = np.sqrt(6) / np.sqrt(n_y + n_h2), dtype=tf.dtypes.float32, seed=None, name=None)\n","    dW1_1 = tf.zeros((n_x, n_h1))\n","    dW1_2 = tf.zeros((n_x, n_h1))\n","    dW2_1 = tf.zeros((n_h1, n_h2))\n","    dW2_2 = tf.zeros((n_h1, n_h2))\n","    dW3_1 = tf.zeros((n_h2, n_y))\n","    dW3_2 = tf.zeros((n_h2, n_y))\n","\n","    g_sum = {\"dW1_1\": dW1_1,\n","             \"dW1_2\": dW1_2,\n","             \"dW2_1\": dW2_1,\n","             \"dW2_2\": dW2_2,\n","             \"dW3_1\": dW3_1,\n","             \"dW3_2\": dW3_2\n","             }\n","    parameters = {\"W1\": W1,\n","                  \"W2\": W2,\n","                  \"W3\": W3,\n","                  \"idx1\": np.nonzero(tf.ones((n_x, 1)) )[0],\n","                  \"idx2\": np.nonzero(tf.ones((n_h1, 1)) )[0],  # initial\n","                  \"idx3\": np.nonzero(tf.ones((n_h2, 1)) )[0]  # initial\n","                  }\n","    pa_in = 1.e-7 * np.ones((4, 1))  # 每一层罚参数初始值\n","    pa_in[3] = 1 / (2 * batch_size)  # 每一层罚参数初始值\n","    tau_in = 1.e-6 * np.ones((4, 1))  # 每一层罚参数初始值\n","    return parameters, pa_in, tau_in, g_sum\n","\n","\n","# 存储参数\n","def save_parameters(parameters, directory):\n","    for key, val in parameters.items():\n","        if not os.path.exists(os.path.join(directory, str(key) + '.npy')):\n","          os.makedirs(os.path.join(directory, str(key) + '.npy'))\n","        tf.save(os.path.join(directory, str(key) + '.npy'), val)\n","\n","\n","def costloss(V3, Y):\n","    cost = tf.square(tf.norm(V3 - Y)) / 2\n","    return cost\n","\n","def penaltyloss(P, C, X, Y):\n","    cost1 = pa[0] * tf.square( tf.norm(Y - hardmax(C[\"U3\"])) ) \n","    cost2 = tau[3] * tf.square(tf.norm( C[\"U3\"] - tf.matmul(tf.transpose(P['W3']),  C[\"V2\"]))  )\n","    cost3 = pa[2] * tf.square( tf.norm(C[\"V2\"] - sgn(C[\"U2\"])) ) + tau[2] * tf.square( tf.norm(C[\"U2\"] - tf.matmul(tf.transpose(P['W2']), C[\"V1\"])) )\n","    cost4 =  pa[1] * tf.square( tf.norm(C[\"V1\"] - sgn(C[\"U1\"])) ) + tau[1] * tf.square( tf.norm(C[\"U1\"] - tf.matmul(tf.transpose(P['W1']), X)) )\n","    cost5 = gam * (tf.square( tf.norm(P['W3'])) + tf.square(tf.norm(P['W2'])) + tf.square(tf.norm(P['W1'])))\n","    \n","    cost = {\"cost1\": cost1.numpy(),\n","             \"cost2\": cost2.numpy(),\n","             \"cost3\": cost3.numpy(),\n","             \"cost4\": cost4.numpy(),\n","             \"cost5\": cost5.numpy(),\n","             \"costall\": cost1.numpy() + cost2.numpy() + cost3.numpy()+cost4.numpy()+cost5.numpy()\n","             }\n","    return cost\n","\n","def sgn(x):\n","    s = tf.sign(x * tf.cast(x > 0, dtype=tf.float32))\n","    return s\n","\n","\n","def softmax(x):  # 返回第一个最大值的位置\n","    v = tf.argmax(x)\n","    return v\n","\n","def hardmax(x):  # 返回矩阵值 dimision: 10 * 64\n","    v1 = np.zeros(x.shape)\n","    v = np.argmax(x, 0)  # tf.argmax(a,1) 每一列最大值位置\n","    for i in range(x.shape[1]):\n","        v1[v[i], i] = 1\n","\n","    return tf.convert_to_tensor(v1, dtype=tf.float32)\n","\n","# 将原矩阵拉成784列但不知道多少行（-1）的矩阵（np.会自动计算配套维数）\n","def image2vector(image):\n","    v = np.reshape(image, [-1, n_x])\n","    return v.T\n","\n","\n","def mini_batchs_generator(inputs, targets, batch_size):\n","    inputs_data_size = len(inputs)\n","    targets_data_size = len(targets)\n","    # print(\"len(inputs):={}, size of inputs {}, size of targets{}\".format(inputs_data_size, inputs.shape, targets.shape))\n","    assert inputs_data_size == targets_data_size, \"The length of inputs({}) and targets({}) must be consistent\".format(\n","        inputs_data_size, targets_data_size)\n","\n","    shuffled_input, shuffled_target = shuffle(inputs, targets)  # 元素随机排序？\n","    mini_batches = [(shuffled_input[k: k + batch_size], shuffled_target[k: k + batch_size])\n","                    for k in range(0, inputs_data_size, batch_size)]  #\n","    for x, y in mini_batches:\n","        # print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n","        # print(x.shape)\n","        yield x, y  # yield = return的功能 + 未终止再使用直到循环完毕(实现一个epoch功能)\n","\n","def produce_step_adam(grad_1, grad_2, g_new, epsilon, delta, rho_1, rho_2, step_ad):\n","    \"\"\"\n","    adam\n","     (i) 梯度滑动-一阶矩  g = rho_1 * grad_1 + (1-rho_1)*g_new\n","     (ii)二阶矩  r_t = rho_2 * grad + (1-rho_1)*g_new*g_new\n","     (iii)更新  step = epsilon /sqrt(delta+r_t)\n","    \"\"\"\n","    grad_1 = rho_1 * grad_1 + (1 - rho_1) * g_new\n","    g = grad_1 / (1 - pow(rho_1, step_ad))\n","    r_t = rho_2 * grad_2 + (1 - rho_2) * g_new * g_new\n","    r = r_t / (1 - pow(rho_2, step_ad))\n","    step = epsilon / tf.sqrt(delta + r)\n","    return step, g, grad_1, r_t\n","\n","\n","def prox_l0_pre(x, sparse, vec_yn):\n","    k = int((x.shape[0] * x.shape[1]) * 0.5) if vec_yn else int(x.shape[0] * 0.51)\n","    if vec_yn:\n","        if k > 5:\n","            abs_x = tf.abs(x)\n","            thre0 = np.partition(abs_x, kth=k, axis=None)[k]\n","            thre = tf.convert_to_tensor(thre0, dtype=tf.float32)\n","            x *= (abs_x >= thre)\n","    else:\n","        x = tf.linalg.norm(x, axis=1, keepdims=True)\n","        # print(np.partition(x, kth=k, axis=None)[k], np.sqrt(2 * 0.13 * sparse * lr))\n","        x *= x >= tf.sqrt(2 * sparse * lr)\n","        # if k > 5:\n","        #     thre = np.partition(x, kth=k, axis=None)[k]\n","        #     x *= (x >= thre)\n","    return x\n","\n","\n","def forward_propagation_pre(x_tol, parameters_for):\n","    layers_num = 3\n","    cache_for = {}\n","    layer_input = x_tol\n","    for layer in range(1, layers_num + 1):\n","        locals()['A' + str(layer)] = tf.matmul(tf.transpose(parameters_for['W' + str(layer)]),\n","                                            layer_input)\n","        cache_for['A' + str(layer)] = locals()['A' + str(layer)]\n","        locals()['Z' + str(layer)] = locals()['A' + str(layer)] * tf.cast(locals()['A' + str(layer)] > 0, dtype=tf.float32)\n","        layer_input = locals()['Z' + str(layer)]\n","        cache_for['Z' + str(layer)] = locals()['Z' + str(layer)]\n","    return locals()['Z' + str(layers_num)], cache_for\n","\n","def optimizer_adam(parameters, cache, X, Y, g_sum, epsilon, delta, sp0, rho_1, rho_2, step_ad, iht=False):\n","    A1 = cache[\"Z1\"]\n","    A2 = cache[\"Z2\"]\n","    A3 = cache[\"Z3\"]\n","\n","    dZ3 = (A3 - Y)  # * Z3(1-Z3)  # h2 * batch\n","    dZ2 = tf.matmul(parameters[\"W3\"], dZ3) * tf.cast(A2 > 0,dtype=tf.float32)\n","    dZ1 = tf.matmul(parameters[\"W2\"], dZ2) * tf.cast(A1 > 0,dtype=tf.float32)\n","\n","    dW3 = tf.matmul(A2, tf.transpose(dZ3))\n","    dW2 = tf.matmul(A1, tf.transpose(dZ2))\n","    dW1 = tf.matmul(X, tf.transpose(dZ1))\n","\n","    step_W3, dW3, g_sum[\"dW3_1\"], g_sum[\"dW3_2\"] = produce_step_adam(g_sum[\"dW3_1\"], g_sum[\"dW3_2\"], dW3,\n","                                                                     epsilon, delta, rho_1, rho_2, step_ad)\n","    step_W2, dW2, g_sum[\"dW2_1\"], g_sum[\"dW2_2\"] = produce_step_adam(g_sum[\"dW2_1\"], g_sum[\"dW2_2\"], dW2,\n","                                                                     epsilon, delta, rho_1, rho_2, step_ad)\n","    step_W1, dW1, g_sum[\"dW1_1\"], g_sum[\"dW1_2\"] = produce_step_adam(g_sum[\"dW1_1\"], g_sum[\"dW1_2\"], dW1, epsilon,\n","                                                                     delta, rho_1, rho_2, step_ad)\n","\n","    parameters[\"W3\"] -= step_W3 * dW3\n","    parameters[\"W2\"] -= step_W2 * dW2\n","    parameters[\"W1\"] -= step_W1 * dW1\n","\n","    if iht:\n","        parameters[\"W3\"] = prox_l0_pre(parameters[\"W2\"], sp0, vec_yn=1)\n","        parameters[\"W2\"] = prox_l0_pre(parameters[\"W2\"], sp0, vec_yn=1)\n","        parameters[\"W1\"] = prox_l0_pre(parameters[\"W1\"], sp0, vec_yn=1)\n","\n","    return parameters, g_sum\n","\n","\n","# feed forward\n","def forward_propagation(X, parameters):\n","    W1 = parameters[\"W1\"]  # W1 维数 （784, 1000）\n","    W2 = parameters[\"W2\"]\n","    W3 = parameters[\"W3\"]\n","\n","    U1 = tf.matmul(tf.transpose(W1), X)  # + b1 * parameters['gamma1']\n","    # betch_size =16, nx*nx = 784 X为（784，16）, gamma1为nx*nx，X * gamma1的维数和X维数相同\n","    V1 = sgn(U1)  # 实现Relu，V1为 (1000，16)为第1隐层的值\n","    U2 = tf.matmul(tf.transpose(W2), V1)  # + b2    # W2^T 维数 （1000,1000） * parameters['gamma2']\n","    V2 = sgn(U2)  # 实现Relu，V2 (1000，16)为第2隐层的值\n","    U3 = tf.matmul(tf.transpose(W3), V2)  # + b3    # W3^T为 （10，1000） * parameters['gamma3']\n","    V3 = hardmax(U3)  # 输出 维数是10*batch 大小\n","\n","    cache = {\n","        \"V0\": X,\n","        \"U0\": X,\n","        \"U1\": U1,\n","        \"V1\": V1,\n","        \"U2\": U2,\n","        \"V2\": V2,\n","        \"U3\": U3,\n","        \"V3\": V3}\n","\n","    return V3, cache\n","\n","\n","def prox_l0(x, sparse, vec_yn):\n","    k = int(x.shape[0] * sp0)\n","    if vec_yn:\n","        if k > 5:\n","            abs_x = tf.abs(x)\n","            thre = np.partition(abs_x, kth=k, axis=None)[k]\n","            x *= (abs_x >= thre)\n","    else:\n","        abx = np.linalg.norm(x, axis=1, keepdims=True)\n","        thre = np.partition(abx, kth=k, axis=None)[k]\n","        abx *= (abx >= thre)\n","    return abx\n","\n","def save_parameters(parameters, directoryin):\n","    for key, val in parameters.items():\n","        np.save(os.path.join(directoryin, str(key)+'.npy'), val)\n","\n","def load_parameters(directoryin):\n","    W1 = np.load(os.path.join(directoryin, 'W1.npy'))\n","    W2 = np.load(os.path.join(directoryin, 'W2.npy'))\n","    W3 = np.load(os.path.join(directoryin, 'W3.npy'))\n","    parameters = {\"idx1\": np.nonzero(np.ones((W1.shape[0], 1)))[0],\n","                  \"idx2\": np.nonzero(np.ones((W2.shape[0], 1)))[0],  \n","                  \"idx3\": np.nonzero(np.ones((W3.shape[0], 1)))[0],\n","                  \"W1\": tf.convert_to_tensor(W1, dtype=tf.float32),\n","                  \"W2\": tf.convert_to_tensor(W2, dtype=tf.float32),\n","                  \"W3\": tf.convert_to_tensor(W3, dtype=tf.float32)\n","                  }\n","    return parameters\n","\n","\n","if __name__ == '__main__':\n","    data_name = \"MNIST01\"\n","    path_dict = {1: \"01_BCD\"}\n","    directory = (\"/content/drive/MyDrive/PBCDcode/MNIST/data/\")\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","        "]},{"cell_type":"markdown","metadata":{"id":"LWK5HBMyk5Bm"},"source":["# Predeal datas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDYWi3UzdB86"},"outputs":[],"source":["    (train_images_or, train_labels), (test_images_or, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","    # How to train\n","    # y_train = to_categorical(y_train)\n","    # y_test = to_categorical(y_test)\n","    model_CNN = create_model(directory)\n","    # history = model_CNN.fit(x_train, y_train, batch_size=16, epochs=3, validation_split=0.05)\n","    # # model specific\n","    # model_CNN.summary()\n","    # # How to evaluate\n","    # loss, acc = model_CNN.evaluate(x_test, y_test, verbose=2)\n","    # How to feadforward and predict\n","    # x_test_select = x_test\n","    # y_true = y_test\n","    # y_pred = model_CNN.predict(x_test_select)\n","    # print('predict result: ', y_pred.shape)\n","    # print('true result: ', y_true.shape)\n","    layer_name = 'flatten'\n","    intermediate_layer_model = keras.Model(inputs=model_CNN.input,\n","                                          outputs=model_CNN.get_layer(layer_name).output)\n","    train_images = intermediate_layer_model(train_images_or[0:60000,:,:])\n","    test_images = intermediate_layer_model(test_images_or)\n","    train_images = train_images.numpy()\n","    test_images = test_images.numpy()\n","    np.save(os.path.join(directory, \"rawdata/train_images.npy\"), train_images)\n","    np.save(os.path.join(directory, \"rawdata/test_images.npy\"), test_images)\n","    np.save(os.path.join(directory, \"rawdata/train_labels.npy\"), train_labels)\n","    np.save(os.path.join(directory, \"rawdata/test_labels.npy\"), test_labels)"]},{"cell_type":"markdown","metadata":{"id":"oUxdmTzZ0uhW"},"source":["# Load dataset#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lo54UyYqiC7L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651851694446,"user_tz":-60,"elapsed":306,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"}},"outputId":"7f72721c-6ac3-4f66-b708-3736bf87e7f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 576)\n"]}],"source":["    # (train_images_or, train_labels), (test_images_or, test_labels) = tf.keras.datasets.mnist.load_data()\n","    train_images = np.load(os.path.join(directory, \"rawdata/train_images.npy\"))\n","    test_images = np.load(os.path.join(directory, \"rawdata/test_images.npy\"))\n","    train_labels = np.load(os.path.join(directory, \"rawdata/train_labels.npy\"))\n","    test_labels = np.load(os.path.join(directory, \"rawdata/test_labels.npy\"))\n","    # normal parameters\n","    if 1:\n","        print(train_images.shape)\n","        n_x = train_images.shape[1]  # 764--> 578\n","        n_h1 = 1000  # 第一隐层\n","        n_h2 = 1000  # 第二隐层\n","        n_y = 10  # 输出层"]},{"cell_type":"markdown","metadata":{"id":"1B7MLxFFvv2h"},"source":["# **Pretrain steps to start**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rey5Hr817rT7"},"outputs":[],"source":["# adam parameters\n","epsilon = 1.e-3  # adam步长的分子\n","delta = 1.e-8  # adam步长的第一个分母\n","rho_1 = 0.9  # adam 一阶距滑动系数\n","rho_2 = 0.99  # adam 二阶距滑动系数\n","step_ad = 10  # adam 二次下降\n","batch_size = 128 \n","sp0 = 1 - 0.7\n","parameters, _, _, g_sum = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","prenum = 1\n","real_number = np.ceil(len(train_labels)/batch_size)\n","for i in range(prenum):\n","    # print('We are in the Ecope {}:'.format(i)) \n","    # with tqdm(total = real_number) as pbar:\n","    train_data = mini_batchs_generator(train_images, train_labels, batch_size)\n","    for step, (img_train, label_train1) in enumerate(train_data):\n","        real_batch_size = len(label_train1)\n","        label_train = np.zeros((10, real_batch_size))  # 10*N\n","        for m, n in zip(label_train1, np.arange(real_batch_size)):\n","            label_train[int(m)][n] = 1\n","        # imgvector1 = image2vector(img_train)\n","        imgvector = normalize_data(img_train.T)  # 784 *\n","        label_train = tf.convert_to_tensor(label_train, dtype=tf.float32)\n","        feat_train = tf.convert_to_tensor(imgvector, dtype=tf.float32)  # batch_size * feat nums  -->  feat nums * batch_size\n","        output, cache = forward_propagation_pre(feat_train, parameters)\n","        parameters, g_sum = optimizer_adam(parameters, cache, feat_train, label_train, g_sum,\n","                                              epsilon, delta, sp0, rho_1, rho_2, step_ad)\n","save_parameters(parameters, os.path.join(directory, \"preparameter\"))"]},{"cell_type":"code","source":["save_parameters(parameters, os.path.join(directory, \"preparameter\"))"],"metadata":{"id":"PJCoYPjvbbg5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hHrNxGDAn_6"},"source":["# Load PreTraining data #"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgcBsNOg7yhI"},"outputs":[],"source":["batch_size = len(train_labels)\n","trainall = normalize_data(train_images.T)\n","testall = normalize_data(test_images.T)\n","\n","label_train = np.zeros((10, batch_size))  # 10*N\n","for m, n in zip(train_labels, np.arange(batch_size)):\n","    label_train[int(m)][n] = 1\n","label_train = tf.convert_to_tensor(label_train, dtype=tf.float32)\n","trainall = tf.convert_to_tensor(trainall, dtype=tf.float32)  # batch_size * feat nums  -->  feat nums * batch_size\n","\n","label_test = np.zeros((10, 10000))  # 10*10000\n","for m, n in zip(test_labels, np.arange(10000)):\n","    label_test[int(m)][n] = 1\n","label_test = tf.convert_to_tensor(label_test, dtype=tf.float32)\n","testall = tf.convert_to_tensor(testall, dtype=tf.float32)       # batch_size * feat nums  -->  feat nums * batch_size\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"wbGiD_X86pl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc_1w85BbxuF"},"outputs":[],"source":["def optimizer(parameters, pinter, X, Y, epoc):\n","    def tenf(koa):\n","        koa = tf.convert_to_tensor(koa, dtype = tf.float32)\n","        return koa\n","    def subuout(ain, biin, tau_in):\n","        # ain batchsize * 10, bin too Cheaked at January 26th\n","        uf = biin\n","        row, column = np.nonzero(ain)\n","        biin = biin.numpy()\n","        cin = tf.transpose(tf.ones((10, batch_size)) * tf.convert_to_tensor(biin[row, column], dtype = tf.float32))\n","        uin2 = tf.minimum(biin, cin)\n","        condition1 = tf.sqrt(tf.convert_to_tensor(tau_in, dtype = tf.float32)) * tf.linalg.norm(biin - uin2, ord=2, axis=1)  # axis 1 represents row\n","        T1 = (condition1 < 2)\n","        uf = uf.numpy()\n","        uf[T1,:] = uin2.numpy()[T1,:]\n","        # uf = uin2 \n","        return tf.convert_to_tensor(uf.T, dtype = tf.float32)  # tf.transpose(uf)  #\n","\n","\n","    def subuin(ain, biin, tau_in):\n","        epi = 3.e-7\n","        uin = biin.numpy()\n","        condition1 = (biin > 0) & (ain < 0.5) & (ain < (1 - tau_in * biin * biin) / 2)\n","        condition2 = (biin < 0) & (ain > 0.5) & (ain > (1 + tau_in * biin * biin) / 2)\n","        uin[condition1] = 0\n","        uin[condition2] = epi\n","        return tf.convert_to_tensor(uin, dtype = tf.float32)\n","\n","    def subw(win, bin, ain, tau_in, gam_in, epoc, layer):\n","        # Win.shape n_{layer-1}, n_layer\n","        # A.shape n_{layer-1}, N\n","        # B.shape n_{layer}, N\n","        b = 2 * tau_in * tf.matmul(ain, tf.transpose(bin))\n","        A = 2 * tau_in * tf.matmul(ain, tf.transpose(ain)) + gam_in * tf.eye(len(b))\n","        d_W = tf.matmul(A, win) - b  # * gamma~~~~~~~~~~~~\n","        W_try = win - lr * d_W\n","        if layer >1:\n","          if epoc < 20:\n","            win = W_try\n","            idx_T = np.nonzero(tf.ones((win.shape[0], 1)))[0]\n","          else:\n","            W_norm2 = prox_l0(W_try, sp0, vec_yn=0)\n","            idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","            idx_Tbar = np.argwhere(W_norm2 == 0)[:, 0]\n","            win = win.numpy()\n","            ain = ain.numpy()\n","            b2 =  2 * tau_in * tf.matmul(tf.matmul(ain[idx_T], tf.transpose(ain[idx_Tbar])), win[idx_Tbar])\n","            b = tenf(b.numpy()[idx_T]) - b2\n","            if epoc > 20:\n","              win[idx_Tbar] -= 1.e-2 * win[idx_Tbar]\n","              # if epoc > 30:\n","              #     win[idx_Tbar] = 0\n","            A1 = tf.linalg.inv(2 * tau_in * tf.matmul(tenf(ain[idx_T]), tenf(tf.transpose(ain[idx_T]))) + gam_in * tf.eye(len(idx_T)))\n","            win[idx_T] = tf.matmul(A1, b)\n","        else:\n","          if epoc % 12 == 0:\n","            win = W_try\n","          else:\n","            win = tf.matmul(tf.linalg.inv(A), b)\n","          idx_T = np.nonzero(tf.ones((win.shape[0], 1)))[0]\n","        return tf.convert_to_tensor(win, dtype = tf.float32), idx_T  # tf.convert_to_tensor(, dtype = tf.float32)\n","\n","    def subv(vin, idx, ain, bin, cin, pa_in, tau_in):  # 显示解\n","        # print(len(idx), vin.shape)\n","        # vin: 1000x64;  ain:  1000[idx+1]x10;  bin:  10, 64;   cin: 1000[idx+1], 64,\n","        for j in range(1):\n","            b = 2 * tau_in * tf.matmul(ain, bin) + 2 * pa_in * cin\n","            # vin -= lr * (2 * tau_in * tf.matmul(tf.matmul(ain, tf.transpose(ain)), vin) - b)\n","            A1 = tf.linalg.inv(2 * tau_in * tf.matmul(ain, tf.transpose(ain)) + 2 * pa_in * tf.eye(len(b)))\n","            # vin = vin.numpy()\n","            vin = tf.matmul(A1, b)\n","        return vin  # tf.convert_to_tensor(vin, dtype = tf.float32)\n","\n","    for layer in range(3, 0, -1):\n","        if layer == 3:\n","            # Y 10*64;  W3 (1000, 10); V3 (10, 64);\n","            pinter['U' + str(layer)] = \\\n","                subuout(tf.transpose(Y), tf.transpose(tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)])), tau[layer] * pa[layer])\n","            parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], gam, epoc, layer)\n","        else:\n","            pinter['V' + str(layer)] = subv(pinter['V' + str(layer)], parameters[\"idx\" + str(layer + 1)],\n","                                            parameters[\"W\" + str(layer + 1)], pinter['U' + str(layer + 1)],\n","                                            sgn(pinter['U' + str(layer)]), pa[layer], tau[layer+1])  # , parameters[\"idx\" + str(layer + 1)])\n","            parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], gam, epoc, layer)\n","            pinter['U' + str(layer)] = subuin(pinter['V' + str(layer)],\n","                                              tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)]),\n","                                              tau[layer] / pa[layer])\n","    return parameters, pinter"]},{"cell_type":"markdown","metadata":{"id":"pDMKOIPsYL_q"},"source":["# **3 Train 0/1 DNN by PBCD**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9hLbeR8tlOv","outputId":"94830195-d483-42eb-9ad2-28fc7830b0b3","executionInfo":{"status":"ok","timestamp":1651853551984,"user_tz":-60,"elapsed":1771574,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["After [3.5600002e-05] [2.5913621e-05] [0.00064852] [1.3906439e-10] 1.8993425e-05 [0.00072903]\n","Famal train: The :0-th epoch, current super1:0. and the current accuracy in this step: 9921.0\n","Updating best accuracy in this epoch:9921.0\n","After [3.34e-05] [2.5934223e-05] [0.00064987] [2.837683e-10] 1.8990871e-05 [0.00072819]\n","Famal train: The :1-th epoch, current super1:0. and the current accuracy in this step: 9922.0\n","Updating best accuracy in this epoch:9922.0\n","After [3.4999997e-05] [2.1371487e-05] [0.00053816] [1.00220525e-07] 1.899114e-05 [0.00061362]\n","Famal train: The :2-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","Updating best accuracy in this epoch:9925.0\n","After [3.2800002e-05] [2.1452606e-05] [0.00054096] [1.4527469e-08] 1.8994324e-05 [0.00061422]\n","Famal train: The :3-th epoch, current super1:0. and the current accuracy in this step: 9924.0\n","After [3.2599997e-05] [1.8461444e-05] [0.00046996] [2.354869e-10] 1.9000307e-05 [0.00054003]\n","Famal train: The :4-th epoch, current super1:0. and the current accuracy in this step: 9927.0\n","Updating best accuracy in this epoch:9927.0\n","After [3.2e-05] [1.857474e-05] [0.00047347] [9.357224e-10] 1.9009158e-05 [0.00054306]\n","Famal train: The :5-th epoch, current super1:0. and the current accuracy in this step: 9926.0\n","After [3.2999997e-05] [1.6842483e-05] [0.00043381] [2.298325e-10] 1.9020787e-05 [0.00050267]\n","Famal train: The :6-th epoch, current super1:0. and the current accuracy in this step: 9927.0\n","After [3.2e-05] [1.6975397e-05] [0.00043794] [9.2791286e-10] 1.9035224e-05 [0.00050595]\n","Famal train: The :7-th epoch, current super1:0. and the current accuracy in this step: 9927.0\n","After [3.2599997e-05] [1.5665199e-05] [0.000409] [2.277524e-10] 1.905244e-05 [0.00047632]\n","Famal train: The :8-th epoch, current super1:0. and the current accuracy in this step: 9924.0\n","After [3.22e-05] [1.5814805e-05] [0.00041362] [9.491461e-10] 1.907241e-05 [0.00048071]\n","Famal train: The :9-th epoch, current super1:0. and the current accuracy in this step: 9926.0\n","After [3.2999997e-05] [1.4531702e-05] [0.00038452] [1.00240094e-07] 1.9095136e-05 [0.00045125]\n","Famal train: The :10-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.24e-05] [1.4692773e-05] [0.00038964] [1.17481235e-07] 1.9120569e-05 [0.00045597]\n","Famal train: The :11-th epoch, current super1:0. and the current accuracy in this step: 9924.0\n","After [3.32e-05] [1.3625887e-05] [0.00036585] [1.00142195e-07] 1.91206e-05 [0.00043189]\n","Famal train: The :12-th epoch, current super1:0. and the current accuracy in this step: 9926.0\n","After [3.2800002e-05] [1.3797337e-05] [0.00037121] [1.398824e-08] 1.9148752e-05 [0.00043697]\n","Famal train: The :13-th epoch, current super1:0. and the current accuracy in this step: 9926.0\n","After [3.2599997e-05] [1.3060184e-05] [0.0003552] [2.4521252e-10] 1.9179639e-05 [0.00042004]\n","Famal train: The :14-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.24e-05] [1.32452415e-05] [0.00036085] [9.732053e-10] 1.9213143e-05 [0.00042571]\n","Famal train: The :15-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.4e-05] [1.5956045e-06] [3.8419636e-05] [2.637344e-10] 1.9249326e-05 [9.3264825e-05]\n","Famal train: The :16-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.36e-05] [1.6059306e-06] [3.873186e-05] [1.1617113e-09] 1.9288107e-05 [9.322706e-05]\n","Famal train: The :17-th epoch, current super1:0. and the current accuracy in this step: 9924.0\n","After [3.34e-05] [1.561567e-06] [3.7565424e-05] [2.4452373e-10] 1.9329556e-05 [9.18568e-05]\n","Famal train: The :18-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.34e-05] [1.5716347e-06] [3.787212e-05] [1.0089296e-09] 1.9373576e-05 [9.2218346e-05]\n","Famal train: The :19-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.4e-05] [1.136101e-06] [2.5685744e-05] [2.519171e-10] 1.9418196e-05 [8.0240294e-05]\n","Famal train: The :20-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.3800003e-05] [1.6846325e-06] [3.885953e-05] [2.2691859e-08] 1.9320089e-05 [9.368695e-05]\n","Famal train: The :21-th epoch, current super1:0. and the current accuracy in this step: 9924.0\n","After [3.32e-05] [1.7031941e-06] [3.8687318e-05] [2.1813491e-08] 1.9225285e-05 [9.283761e-05]\n","Famal train: The :22-th epoch, current super1:0. and the current accuracy in this step: 9926.0\n","After [3.2800002e-05] [3.2482067e-06] [7.5931704e-05] [1.6741261e-07] 1.9137946e-05 [0.00013129]\n","Famal train: The :23-th epoch, current super1:0. and the current accuracy in this step: 9929.0\n","Updating best accuracy in this epoch:9929.0\n","After [3.2599997e-05] [1.6917164e-06] [3.7763413e-05] [2.194403e-08] 1.8997998e-05 [9.107507e-05]\n","Famal train: The :24-th epoch, current super1:0. and the current accuracy in this step: 9926.0\n","After [3.2599997e-05] [3.1956884e-06] [7.3705574e-05] [1.683339e-07] 1.8919252e-05 [0.00012859]\n","Famal train: The :25-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.32e-05] [1.6920449e-06] [3.7392765e-05] [2.1752587e-08] 1.884426e-05 [9.1150825e-05]\n","Famal train: The :26-th epoch, current super1:0. and the current accuracy in this step: 9927.0\n","After [3.2800002e-05] [3.1500363e-06] [7.202252e-05] [2.6707025e-07] 1.877638e-05 [0.00012702]\n","Famal train: The :27-th epoch, current super1:0. and the current accuracy in this step: 9928.0\n","After [3.3800003e-05] [1.6692084e-06] [3.6423357e-05] [2.1744553e-08] 1.8711717e-05 [9.062603e-05]\n","Famal train: The :28-th epoch, current super1:0. and the current accuracy in this step: 9925.0\n","After [3.36e-05] [3.0678166e-06] [6.969722e-05] [1.6693627e-07] 1.8654584e-05 [0.00012519]\n","Famal train: The :29-th epoch, current super1:0. and the current accuracy in this step: 9930.0\n","Updating best accuracy in this epoch:9930.0\n","After [3.34e-05] [1.6385686e-06] [3.5476154e-05] [2.173533e-08] 1.8599998e-05 [8.9136454e-05]\n","Famal train: The :30-th epoch, current super1:0. and the current accuracy in this step: 9931.0\n","Updating best accuracy in this epoch:9931.0\n","After [3.36e-05] [1.6244956e-06] [3.5085177e-05] [2.1325311e-08] 1.8550341e-05 [8.888134e-05]\n","Famal train: The :31-th epoch, current super1:0. and the current accuracy in this step: 9931.0\n","After [3.34e-05] [1.6117248e-06] [3.428682e-05] [2.1164256e-08] 1.8505563e-05 [8.7825276e-05]\n","Famal train: The :32-th epoch, current super1:0. and the current accuracy in this step: 9930.0\n","After [3.2800002e-05] [1.5966289e-06] [3.3773813e-05] [2.1133546e-08] 1.8465584e-05 [8.6657165e-05]\n","Famal train: The :33-th epoch, current super1:0. and the current accuracy in this step: 9931.0\n","After [3.2800002e-05] [1.577343e-06] [3.3032775e-05] [2.1407788e-08] 1.8430368e-05 [8.586189e-05]\n","Famal train: The :34-th epoch, current super1:0. and the current accuracy in this step: 9931.0\n","After [3.36e-05] [1.5770145e-06] [3.285388e-05] [2.0880277e-08] 1.8399833e-05 [8.645161e-05]\n","Famal train: The :35-th epoch, current super1:0. and the current accuracy in this step: 9932.0\n","Updating best accuracy in this epoch:9932.0\n","After [3.36e-05] [1.574257e-06] [3.244315e-05] [2.0884125e-08] 1.8290246e-05 [8.5928536e-05]\n","Famal train: The :36-th epoch, current super1:0. and the current accuracy in this step: 9931.0\n","After [3.3800003e-05] [1.5606932e-06] [3.193892e-05] [2.0740336e-08] 1.8266577e-05 [8.558693e-05]\n","Famal train: The :37-th epoch, current super1:0. and the current accuracy in this step: 9934.0\n","Updating best accuracy in this epoch:9934.0\n","After [3.32e-05] [1.5426394e-06] [3.132294e-05] [2.0809185e-08] 1.8247447e-05 [8.4333835e-05]\n","Famal train: The :38-th epoch, current super1:0. and the current accuracy in this step: 9934.0\n","After [3.4200002e-05] [1.5269987e-06] [3.0787694e-05] [2.1145178e-08] 1.8232779e-05 [8.476862e-05]\n","Famal train: The :39-th epoch, current super1:0. and the current accuracy in this step: 9932.0\n","After [3.4e-05] [1.5136375e-06] [3.0319272e-05] [2.0742654e-08] 1.8222518e-05 [8.407617e-05]\n","Famal train: The :40-th epoch, current super1:0. and the current accuracy in this step: 9929.0\n","After [3.4e-05] [1.5036014e-06] [2.996559e-05] [2.0908846e-08] 1.8216611e-05 [8.370671e-05]\n","Famal train: The :41-th epoch, current super1:0. and the current accuracy in this step: 9931.0\n","After [3.4600005e-05] [1.4971539e-06] [2.9629746e-05] [2.0303087e-08] 1.8215002e-05 [8.396221e-05]\n","Famal train: The :42-th epoch, current super1:0. and the current accuracy in this step: 9929.0\n","After [3.52e-05] [1.4897631e-06] [2.9328938e-05] [2.0278387e-08] 1.8217626e-05 [8.425661e-05]\n","Famal train: The :43-th epoch, current super1:0. and the current accuracy in this step: 9931.0\n","After [3.48e-05] [1.478211e-06] [2.8926248e-05] [2.0437954e-08] 1.822446e-05 [8.344936e-05]\n","Famal train: The :44-th epoch, current super1:0. and the current accuracy in this step: 9933.0\n","After [3.4600005e-05] [1.4798924e-06] [2.889311e-05] [2.0283712e-08] 1.823543e-05 [8.322872e-05]\n","Famal train: The :45-th epoch, current super1:0. and the current accuracy in this step: 9933.0\n","After [3.5600002e-05] [1.467344e-06] [2.8401924e-05] [1.2015566e-07] 1.8250468e-05 [8.383989e-05]\n","Famal train: The :46-th epoch, current super1:0. and the current accuracy in this step: 9933.0\n","After [3.5799996e-05] [1.456969e-06] [2.8030296e-05] [1.9810226e-08] 1.8269526e-05 [8.35766e-05]\n","Famal train: The :47-th epoch, current super1:0. and the current accuracy in this step: 9934.0\n","After [3.5600002e-05] [1.4304676e-06] [2.7504067e-05] [2.0089905e-08] 1.8183813e-05 [8.2738436e-05]\n","Famal train: The :48-th epoch, current super1:0. and the current accuracy in this step: 9931.0\n","After [3.5600002e-05] [1.4153223e-06] [2.6999625e-05] [1.984787e-08] 1.8208622e-05 [8.224342e-05]\n","Famal train: The :49-th epoch, current super1:0. and the current accuracy in this step: 9928.0\n","========================================================\n","=======================New accuracy=====================\n","The current best acc of all is: 9934.0, super parameter1 is: 0,\n","========================================================\n","The final best acc of all is: 9934.0, super parameter1 is: 0:\n"]}],"source":["batch_size = len(train_labels)\n","history1 = np.zeros((100,1))\n","history2 = np.zeros((100,1))\n","super1_all = [0]  # current best parameter 1.e-4:\n","epochs = 50\n","low_acc = -9800\n","best_acc_of_all = 0  # 1.2\n","best_para1 = 0  # epi573 = 0.01\n","for super1 in super1_all:\n","    sp0 = 1 - 0.3   # sparsity 0.7\n","    lr = 7.2  # 6.e-2 ~ 6.e-1  turned at Feb 17th\n","    ship = 2.e-2  # slippage for the next step\n","    gam = 1.e-8  # turned at Feb 27th 6.e-2  --->  6.e-3   gamma for penalty norm2 \"W\"\n","    best_acc = 0\n","    \n","    parameters = load_parameters(os.path.join(directory, \"preparameter\"))\n","    # parameters, pa, tau, _ = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","\n","    pa = 1.e-7 * np.ones((4, 1))  # 每一层罚参数初始值\n","    pa[3] = 1 / (2 * batch_size)  # 每一层罚参数初始值\n","    tau = 1.e-6 * np.ones((4, 1))  # 每一层罚参数初始值\n","    # try:   \n","    for i in range(epochs):\n","        aa25, _ = forward_propagation(trainall, parameters)\n","        acc = (60000 - costloss(aa25, label_train))/6\n","        history1[i] = acc\n","\n","        if i % 2 == 0 or i > 30:\n","          aa21, cache = forward_propagation(trainall, parameters)\n","        \n","\n","        parameters, cache = optimizer(parameters, cache, trainall, label_train, i)\n","        \n","        if i % 1 == 0:\n","          merit = penaltyloss(parameters, cache, trainall, label_train)\n","          # acc_train = costloss(aa21, label_train)\n","          print(\"After\", merit[\"cost1\"], merit[\"cost2\"], merit[\"cost3\"], merit[\"cost4\"], \n","                merit[\"cost5\"], merit[\"costall\"])\n","        if merit[\"cost1\"] < 0.7 * merit[\"cost5\"]:\n","          gam *= 0.5\n","        if i % 15 == 0 and i>2:\n","          lr *= 0.2\n","\n","        aa25, _ = forward_propagation(testall, parameters)\n","        acc = 10000 - costloss(aa25, label_test)\n","        history2[i] = acc\n","\n","        if acc < low_acc or merit[\"costall\"] > 10000:\n","            print(\"Too low acc:{} or merit:{}.\".format(acc, merit[\"costall\"]))\n","            break\n","        else:\n","            print('Famal train: The :{}-th epoch, current super1:{}. and the current accuracy in this step: {}'.format(\n","                    i, super1, acc))\n","        if acc > best_acc:\n","            best_acc = acc\n","            print('Updating best accuracy in this epoch:{}'.format(best_acc))\n","\n","    if best_acc > best_acc_of_all:\n","        best_acc_of_all = best_acc\n","        best_para1 = super1\n","        print(\"========================================================\")\n","        print(\"=======================New accuracy=====================\")\n","        print(\"The current best acc of all is: {}, super parameter1 is: {},\".format(\n","            best_acc_of_all, best_para1))\n","        print(\"========================================================\")\n","    # except:\n","    #     print(\"There are some wrong things, but pass.\")\n","    #     continue   # break\n","\n","print(\"The final best acc of all is: {}, super parameter1 is: {}:\".format( best_acc_of_all, best_para1))"]},{"cell_type":"code","source":["np.save(os.path.join(directory, \"result/history1.npy\"), history1)\n","np.save(os.path.join(directory, \"result/history2.npy\"), history2)"],"metadata":{"id":"4wdfbrIcG6S7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1645891896602,"user":{"displayName":"Zhang Hui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06808414998828253018"},"user_tz":0},"id":"wvPh4H_djBaL","outputId":"201c6ec8-2dc6-4473-843e-351bd7ec273d"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[ 0.6790364   0.2636888   0.04981804  0.7284305   0.45073485]\n"," [-0.77727365 -0.62130547  0.42814064  0.26005507 -0.99393487]\n"," [-0.03302193  0.63740945 -0.04959607  0.5901475   0.3933952 ]], shape=(3, 5), dtype=float32) tf.Tensor(\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]], shape=(3, 3), dtype=float32)\n"]}],"source":["#from torch._C import float32\n","# import tensorflow as tf\n","# import torch as tc\n","# import numpy as np\n","# from tqdm import tqdm, trange\n","# import time\n","\n","# import time\n","# from tqdm import tqdm, trange\n","\n","\n","a1 = tf.random.uniform([3,5], minval=-1, maxval=1, dtype=tf.dtypes.float32, seed=None, name=None)\n","a2 =  3 * tf.eye(3)\n","a3 = hardmax(a1)\n","x = a2\n","print(a1, tf.sign(x * tf.cast(x >= 0, dtype=tf.float32)))\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"MNIST01.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
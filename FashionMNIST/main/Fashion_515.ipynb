{"cells":[{"cell_type":"markdown","metadata":{"id":"tT2VMZJXYhTw"},"source":["1. Version  : 1\n","2. Time     : 5th May\n","3. Author   : Hui Zhang\n","4. Function : PBCD method to solve 2 hidden layer 0/1 DNN on Fashion MNIST\n","5. Structure: 2 hidden layer with 2000 nodes in each layer\n","6. Relation paper: PBCD method for 0/1 DNN"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1892,"status":"ok","timestamp":1652897458799,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"},"user_tz":-60},"id":"5L1Ah48Ll3Nu","outputId":"d72aa4fa-f1bc-48ec-e36d-68c68bb7e71e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# drive.mount(\"/content/drive/\", force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"CXfp0lN9YW3v"},"source":["# **Define relavant function and load data**"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1338,"status":"ok","timestamp":1652897460130,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"},"user_tz":-60},"id":"kh7W8cowv1-6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbcdebe8-242b-4111-fc0b-17b3b339984d"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 576) 60000\n"]}],"source":["'''\n","    Function: 10 Classification for fashion\n","    Version: Feb18th-version**\n","    Network: 2 hidden layer with 2000 neurons, 0/1 actativation + hardmax\n","    Time: 2022.02.18\n","'''\n","\n","import numpy as np   # science calculate  instill numpy\n","import struct  # y???????\n","from sklearn.utils import shuffle  # instill scikit-learn/ scikit\n","import time  \n","import os  # o??????????\n","import scipy\n","import torch as tc\n","import tensorflow as tf\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm, trange\n","import tensorflow.keras as keras\n","from tensorflow.keras import datasets\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense,Conv2D,Dropout,Flatten,Activation,BatchNormalization,AveragePooling2D,MaxPooling2D\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","# Standardized data in 0-1\n","def normalize_data(ima):\n","    a_max = np.max(ima)  # 图像矩阵的最大值\n","    a_min = np.min(ima)  # 图像矩阵的最小值\n","    for i in range(ima.shape[0]):  # 矩阵第二维的长度：如3*4矩阵这个就是3\n","        for j in range(ima.shape[1]):  # numpy.core.fromnumeric 中的函数 第一维的长度\n","            ima[i][j] = (ima[i][j] - a_min) / (a_max - a_min)\n","    return ima  # 返回是一个矩阵\n","\n","# 初始化各个参数 n_x x的维数 n_h1：第一隐层节点数（靠近x的）, n_h2：第二隐层节点数, n_y： 输出y的维数\n","def initialize_with_zeros(n_x, n_h1, n_h2, n_y):\n","    tf.random.set_seed(21)\n","    W1 = tf.random.uniform([n_x, n_h1], minval = -np.sqrt(6) / np.sqrt(n_x + n_h1), maxval = np.sqrt(6) / np.sqrt(n_h1 + n_x), dtype=tf.dtypes.float32, seed=None, name=None)\n","    # b1 = tf.zeros((n_h1, 1))\n","    # 可选取不同的 gamma1 = np.random.uniform(low=-0.5, high=0.5, size=(n_x, 1))\n","    # gamma1 = tf.ones((n_x, 1))  # np.random.randn(n_x, 1)\n","\n","    W2 = tf.random.uniform([n_h1, n_h2], minval = -np.sqrt(6) / np.sqrt(n_h1 + n_h2), maxval = np.sqrt(6) / np.sqrt(n_h1 + n_h2), dtype=tf.dtypes.float32, seed=None, name=None)\n","    # gamma2 = tf.ones((n_h1, 1))  #\n","\n","    W3 = tf.random.uniform([n_h2, n_y], minval = -np.sqrt(6) / np.sqrt(n_y + n_h2), maxval = np.sqrt(6) / np.sqrt(n_y + n_h2), dtype=tf.dtypes.float32, seed=None, name=None)\n","    # b3 = tf.zeros((n_y, 1))\n","    # gamma3 = tf.ones((n_h2, 1))  # \n","\n","    dW1_1 = tf.zeros((n_x, n_h1))\n","    dW1_2 = tf.zeros((n_x, n_h1))\n","    dW2_1 = tf.zeros((n_h1, n_h2))\n","    dW2_2 = tf.zeros((n_h1, n_h2))\n","    dW3_1 = tf.zeros((n_h2, n_y))\n","    dW3_2 = tf.zeros((n_h2, n_y))\n","\n","    g_sum = {\"dW1_1\": dW1_1,\n","             \"dW1_2\": dW1_2,\n","             \"dW2_1\": dW2_1,\n","             \"dW2_2\": dW2_2,\n","             \"dW3_1\": dW3_1,\n","             \"dW3_2\": dW3_2\n","             }\n","    parameters = {\"W1\": W1,\n","                  \"W2\": W2,\n","                  \"W3\": W3,\n","                  \"idx1\": np.ones((n_x, 1)),  # initial\n","                  \"idx2\": np.ones((n_h1, 1)),  # initial\n","                  \"idx3\": np.ones((n_h2, 1))  # initial\n","                  }\n","    pa_in = 1.e-7 * np.ones((4, 1))  # 每一层罚参数初始值\n","    pa_in[3] = 1 / (2 * batch_size)  # 每一层罚参数初始值\n","    tau_in = 1.e-6 * np.ones((4, 1))  # 每一层罚参数初始值\n","    return parameters, pa_in, tau_in, g_sum\n","\n","def costloss(V3, Y):\n","    cost = tf.square(tf.norm(V3 - Y)) / 2\n","    return cost\n","\n","def penaltyloss(P, C, X, Y):\n","    cost1 = pa[3] * tf.square( tf.norm(Y - hardmax(C[\"U3\"])) ) \n","    cost2 = tau[3] * tf.square(tf.norm( C[\"U3\"] - tf.matmul(tf.transpose(P['W3']),  C[\"V2\"]))  )\n","    cost3 = pa[2] * tf.square( tf.norm(C[\"V2\"] - sgn(C[\"U2\"])) ) + tau[2] * tf.square( tf.norm(C[\"U2\"] - tf.matmul(tf.transpose(P['W2']), C[\"V1\"])) )\n","    print(\"Cheack out, now-------------------\")\n","    print(pa[2], (tf.square( tf.norm(C[\"V2\"] - sgn(C[\"U2\"])) )).numpy(), tau[2], (tf.square( tf.norm(C[\"U2\"] - tf.matmul(tf.transpose(P['W2']), C[\"V1\"])) )).numpy())\n","    print(\"------------------------------------\")\n","    cost4 =  pa[1] * tf.square( tf.norm(C[\"V1\"] - sgn(C[\"U1\"])) ) + tau[1] * tf.square( tf.norm(C[\"U1\"] - tf.matmul(tf.transpose(P['W1']), X)) )\n","    cost5 = gam * (tf.square( tf.norm(P['W3'])) + tf.square(tf.norm(P['W2'])) + tf.square(tf.norm(P['W1'])))\n","    \n","    cost = {\"cost1\": cost1.numpy(),\n","             \"cost2\": cost2.numpy(),\n","             \"cost3\": cost3.numpy(),\n","             \"cost4\": cost4.numpy(),\n","             \"cost5\": cost5.numpy(),\n","             \"costall\": cost1.numpy() + cost2.numpy() + cost3.numpy()+cost4.numpy()+cost5.numpy()\n","             }\n","    return cost\n","\n","def sgn(x):\n","    s = tf.sign(x * tf.cast(x > 0, dtype=tf.float32))\n","    return s\n","\n","\n","def softmax(x):  # 返回第一个最大值的位置\n","    v = tf.argmax(x)\n","    return v\n","\n","def hardmax(x):  # 返回矩阵值 dimision: 10 * 64\n","    v1 = np.zeros(x.shape)\n","    v = np.argmax(x, 0)  # tf.argmax(a,1) 每一列最大值位置\n","    for i in range(x.shape[1]):\n","        v1[v[i], i] = 1\n","\n","    return tf.convert_to_tensor(v1, dtype=tf.float32)\n","\n","# 将原矩阵拉成784列但不知道多少行（-1）的矩阵（np.会自动计算配套维数）\n","def image2vector(image):\n","    v = np.reshape(image, [-1, n_x])\n","    return v.T\n","\n","\n","def mini_batchs_generator(inputs, targets, batch_size):\n","    inputs_data_size = len(inputs)\n","    targets_data_size = len(targets)\n","    # print(\"len(inputs):={}, size of inputs {}, size of targets{}\".format(inputs_data_size, inputs.shape, targets.shape))\n","    assert inputs_data_size == targets_data_size, \"The length of inputs({}) and targets({}) must be consistent\".format(\n","        inputs_data_size, targets_data_size)\n","\n","    shuffled_input, shuffled_target = shuffle(inputs, targets)  # 元素随机排序？\n","    mini_batches = [(shuffled_input[k: k + batch_size], shuffled_target[k: k + batch_size])\n","                    for k in range(0, inputs_data_size, batch_size)]  #\n","    for x, y in mini_batches:\n","        # print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n","        # print(x.shape)\n","        yield x, y  # yield = return的功能 + 未终止再使用直到循环完毕(实现一个epoch功能)\n","\n","\n","\n","def produce_step_adam(grad_1, grad_2, g_new, epsilon, delta, rho_1, rho_2, step_ad):\n","    \"\"\"\n","    adam\n","     (i) 梯度滑动-一阶矩  g = rho_1 * grad_1 + (1-rho_1)*g_new\n","     (ii)二阶矩  r_t = rho_2 * grad + (1-rho_1)*g_new*g_new\n","     (iii)更新  step = epsilon /sqrt(delta+r_t)\n","    \"\"\"\n","    grad_1 = rho_1 * grad_1 + (1 - rho_1) * g_new\n","    g = grad_1 / (1 - pow(rho_1, step_ad))\n","    r_t = rho_2 * grad_2 + (1 - rho_2) * g_new * g_new\n","    r = r_t / (1 - pow(rho_2, step_ad))\n","    step = epsilon / tf.sqrt(delta + r)\n","    return step, g, grad_1, r_t\n","\n","\n","def prox_l0_pre(x, sparse, vec_yn):\n","    k = int((x.shape[0] * x.shape[1]) * 0.5) if vec_yn else int(x.shape[0] * 0.51)\n","    if vec_yn:\n","        if k > 5:\n","            abs_x = tf.abs(x)\n","            thre0 = np.partition(abs_x, kth=k, axis=None)[k]\n","            thre = tf.convert_to_tensor(thre0, dtype=tf.float32)\n","            x *= (abs_x >= thre)\n","    else:\n","        x = tf.linalg.norm(x, axis=1, keepdims=True)\n","        # print(np.partition(x, kth=k, axis=None)[k], np.sqrt(2 * 0.13 * sparse * lr))\n","        x *= x >= tf.sqrt(2 * sparse * lr)\n","        # if k > 5:\n","        #     thre = np.partition(x, kth=k, axis=None)[k]\n","        #     x *= (x >= thre)\n","    return x\n","\n","\n","def forward_propagation_pre(x_tol, parameters_for):\n","    layers_num = 3\n","    cache_for = {}\n","    layer_input = x_tol\n","    for layer in range(1, layers_num + 1):\n","        locals()['A' + str(layer)] = tf.matmul(tf.transpose(parameters_for['W' + str(layer)]),\n","                                            layer_input)\n","        cache_for['A' + str(layer)] = locals()['A' + str(layer)]\n","        locals()['Z' + str(layer)] = locals()['A' + str(layer)] * tf.cast(locals()['A' + str(layer)] > 0, dtype=tf.float32)\n","        layer_input = locals()['Z' + str(layer)]\n","        cache_for['Z' + str(layer)] = locals()['Z' + str(layer)]\n","    return locals()['Z' + str(layers_num)], cache_for\n","\n","\n","def optimizer_adam_l0(parameters, cache, X, Y, g_sum, epsilon, delta, sp0, rho_1, rho_2, step_ad, iht=False):\n","    A1 = cache[\"Z1\"]\n","    A2 = cache[\"Z2\"]\n","    A3 = cache[\"Z3\"]\n","\n","    dZ3 = (A3 - Y)  # * Z3(1-Z3)  # h2 * batch\n","    dZ2 = tf.matmul(parameters[\"W3\"], dZ3) * tf.cast(A2 > 0,dtype=tf.float32)\n","    dZ1 = tf.matmul(parameters[\"W2\"], dZ2) * tf.cast(A1 > 0,dtype=tf.float32)\n","\n","    dW3 = tf.matmul((A2), tf.transpose(dZ3))\n","    dW2 = tf.matmul((A1), tf.transpose(dZ2))\n","    dW1 = tf.matmul((X), tf.transpose(dZ1))\n","\n","    step_W3, dW3, g_sum[\"dW3_1\"], g_sum[\"dW3_2\"] = produce_step_adam(g_sum[\"dW3_1\"], g_sum[\"dW3_2\"], dW3,\n","                                                                     epsilon, delta, rho_1, rho_2, step_ad)\n","    step_W2, dW2, g_sum[\"dW2_1\"], g_sum[\"dW2_2\"] = produce_step_adam(g_sum[\"dW2_1\"], g_sum[\"dW2_2\"], dW2,\n","                                                                     epsilon, delta, rho_1, rho_2, step_ad)\n","    step_W1, dW1, g_sum[\"dW1_1\"], g_sum[\"dW1_2\"] = produce_step_adam(g_sum[\"dW1_1\"], g_sum[\"dW1_2\"], dW1, epsilon,\n","                                                                     delta, rho_1, rho_2, step_ad)\n","\n","    parameters[\"W3\"] -= step_W3 * dW3\n","    parameters[\"W2\"] -= step_W2 * dW2\n","    parameters[\"W1\"] -= step_W1 * dW1\n","\n","    if iht:\n","        parameters[\"W3\"] = prox_l0_pre(parameters[\"W2\"], sp0, vec_yn=1)\n","        parameters[\"W2\"] = prox_l0_pre(parameters[\"W2\"], sp0, vec_yn=1)\n","        parameters[\"W1\"] = prox_l0_pre(parameters[\"W1\"], sp0, vec_yn=1)\n","\n","    return parameters, g_sum\n","\n","\n","# feed forward\n","def forward_propagation(X, parameters):\n","    W1 = parameters[\"W1\"]  # W1 维数 （784, 1000）\n","    W2 = parameters[\"W2\"]\n","    W3 = parameters[\"W3\"]\n","\n","    U1 = tf.matmul(tf.transpose(W1), X)  # + b1 * parameters['gamma1']\n","    # betch_size =16, nx*nx = 784 X为（784，16）, gamma1为nx*nx，X * gamma1的维数和X维数相同\n","    V1 = sgn(U1)  # 实现Relu，V1为 (1000，16)为第1隐层的值\n","    U2 = tf.matmul(tf.transpose(W2), V1)  # + b2    # W2^T 维数 （1000,1000） * parameters['gamma2']\n","    V2 = sgn(U2)  # 实现Relu，V2 (1000，16)为第2隐层的值\n","    U3 = tf.matmul(tf.transpose(W3), V2)  # + b3    # W3^T为 （10，1000） * parameters['gamma3']\n","    V3 = hardmax(U3)  # 输出 维数是10*batch 大小\n","\n","    cache = {\n","        \"V0\": X,\n","        \"U0\": X,\n","        \"U1\": U1,\n","        \"V1\": V1,\n","        \"U2\": U2,\n","        \"V2\": V2,\n","        \"U3\": U3,\n","        \"V3\": V3}\n","\n","    return V3, cache\n","\n","\n","def prox_l0(x, sparse, vec_yn):\n","    k = int(x.shape[0] * sp0)\n","    if vec_yn:\n","        if k > 5:\n","            abs_x = tf.abs(x)\n","            thre = np.partition(abs_x, kth=k, axis=None)[k]\n","            x *= (abs_x >= thre)\n","    else:\n","        abx = np.linalg.norm(x, axis=1, keepdims=True)\n","        # print(abx.shape, x.shape[0], k)\n","        thre = np.partition(abx, kth=k, axis=None)[k]\n","        abx *= (abx >= thre)\n","        # print(np.partition(x, kth=k, axis=None)[k], np.sqrt(2 * 0.13 * sparse * lr))\n","        # x *= x >= np.sqrt(2 * sparse * lr)\n","        # if k > 5:\n","        #     thre = np.partition(x, kth=k, axis=None)[k]\n","        #     x *= (x >= thre)\n","    return abx\n","\n","# 存储参数\n","def save_parameters(parameters, directory):\n","    for key, val in parameters.items():\n","        if not os.path.exists(os.path.join(directory, str(key) + '.npy')):\n","          os.makedirs(os.path.join(directory, str(key) + '.npy'))\n","        np.save(os.path.join(directory, str(key) + '.npy'), val)\n","\n","def load_parameters(directoryin):\n","    W1 = np.load(os.path.join(directoryin, 'W1.npy'))\n","    W2 = np.load(os.path.join(directoryin, 'W2.npy'))\n","    W3 = np.load(os.path.join(directoryin, 'W3.npy'))\n","    parameters = {\"idx1\": np.nonzero(np.ones((W1.shape[0], 1)))[0],\n","                  \"idx2\": np.nonzero(np.ones((W2.shape[0], 1)))[0],  \n","                  \"idx3\": np.nonzero(np.ones((W3.shape[0], 1)))[0],\n","                  \"W1\": tf.convert_to_tensor(W1, dtype=tf.float32),\n","                  \"W2\": tf.convert_to_tensor(W2, dtype=tf.float32),\n","                  \"W3\": tf.convert_to_tensor(W3, dtype=tf.float32)\n","                  }\n","    return parameters\n","\n","\n","if __name__ == '__main__':\n","    data_name = \"fashion10\"\n","    path_dict = {1: \"01_BCD\"}\n","    directory = (\"/content/drive/MyDrive/PBCDcode/FashionMNIST/data/\")\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","    \n","    # ============Load dataset\n","    train_images = np.load(os.path.join(directory, \"rawdata/train_images.npy\"))\n","    test_images = np.load(os.path.join(directory, \"rawdata/test_images.npy\"))\n","    train_labels = np.load(os.path.join(directory, \"rawdata/train_labels.npy\"))\n","    test_labels = np.load(os.path.join(directory, \"rawdata/test_labels.npy\"))\n","\n","    tau = 3.e-3  # tau for penalty V~U_01\n","    pa = 5.e-6  # index pai for penalty U~WV\n","    gam = 1.e-5  # gamma for penalty norm2 \"W\"\n","\n","    lr = 0.01  # 0.03 - 0.1   stepsize for choosing index of W\n","    sp0 = 1 - 0.7   # sparsity 0.6\n","    batch_size = len(train_labels)  # 批次大小\n","    # epochs = 20\n","\n","    # normal parameters\n","    if 1:\n","        print(train_images.shape, batch_size)\n","        n_x = train_images.shape[1]  # 764--> \n","        n_h1 = 2000  # 第一隐层\n","        n_h2 = 2000  # 第二隐层\n","        n_y = 10  # 输出层\n","        epoch_times = []  # 记录时间\n","        train_losses = []\n","        test_losses = []\n","        node1s = []\n","        node2s = []\n","        node3s = []\n","        precode = 0  # adam pretrain\n","        # adam parameters\n","        epsilon = 1.e-3  # adam步长的分子\n","        delta = 1.e-8  # adam步长的第一个分母\n","        rho_1 = 0.9  # adam 一阶距滑动系数\n","        rho_2 = 0.99  # adam 二阶距滑动系数\n","        step_ad = 10  # adam 二次下降\n","        # try:\n","        "]},{"cell_type":"markdown","metadata":{"id":"1B7MLxFFvv2h"},"source":["# **Pretrain steps to start**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rey5Hr817rT7"},"outputs":[],"source":["batch_size = 128\n","# require size: (60000, 1024) (10000, 1024) (60000, 10) (10000, 10)\n","n_x = train_images.shape[1]\n","parameters, _, _, g_sum = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","prenum = 2\n","real_number = np.ceil(len(train_labels)/batch_size)\n","for i in range(prenum):\n","    # print('We are in the Ecope {}:'.format(i)) \n","    # with tqdm(total = real_number) as pbar:\n","    train_data = mini_batchs_generator(train_images, train_labels, batch_size)\n","    for step, (img_train, label_train1) in enumerate(train_data):\n","        real_batch_size = len(label_train1)\n","        label_train = label_train1.T\n","        # imgvector1 = image2vector(img_train)\n","        imgvector = normalize_data(img_train.T)  # 784 *\n","        label_train = tf.convert_to_tensor(label_train, dtype=tf.float32)\n","        feat_train = tf.convert_to_tensor(imgvector, dtype=tf.float32)  # batch_size * feat nums  -->  feat nums * batch_size\n","        # print(feat_train.shape, parameters[\"W1\"].shape)\n","        output, cache = forward_propagation_pre(feat_train, parameters)\n","        \n","        parameters, g_sum = optimizer_adam_l0(parameters, cache, feat_train, label_train, g_sum,\n","                                              epsilon, delta, sp0, rho_1, rho_2, step_ad)\n","save_parameters(parameters, os.path.join(directory, \"preparameter\"))"]},{"cell_type":"code","source":["save_parameters(parameters, os.path.join(directory, \"preparameter\"))"],"metadata":{"id":"mzL5V1LcYwEh","executionInfo":{"status":"ok","timestamp":1652897948053,"user_tz":-60,"elapsed":511,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hHrNxGDAn_6"},"source":["# Load PreTraining data #"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ZgcBsNOg7yhI","executionInfo":{"status":"ok","timestamp":1652897984872,"user_tz":-60,"elapsed":33169,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"}}},"outputs":[],"source":["# input shape: (60000, 576) (10000, 576) (60000, 10) (10000, 10)\n","batch_size = 60000\n","\n","trainall = normalize_data(train_images.T)\n","trainall = tf.convert_to_tensor(trainall, dtype=tf.float32)  # batch_size * feat nums  -->  feat nums * batch_size\n","\n","testall = normalize_data(test_images.T)\n","testall = tf.convert_to_tensor(testall, dtype=tf.float32)       # batch_size * feat nums  -->  feat nums * batch_size\n","\n","label_trainall = train_labels.T\n","label_trainall = tf.convert_to_tensor(label_trainall, dtype=tf.float32)\n","\n","label_testall = test_labels.T\n","label_testall = tf.convert_to_tensor(label_testall, dtype=tf.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200,"status":"ok","timestamp":1649435324048,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"},"user_tz":-60},"id":"lni6kUrkwypj","outputId":"cb2f0a95-79b1-48cc-edfe-bfc84dd3ec0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(576, 60000) (576, 10000) (10, 60000) (10, 10000)\n"]}],"source":["print(trainall.shape, testall.shape, label_trainall.shape, label_testall.shape)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Pc_1w85BbxuF","executionInfo":{"status":"ok","timestamp":1652897985141,"user_tz":-60,"elapsed":298,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"}}},"outputs":[],"source":["def optimizer(parameters, pinter, X, Y, epoc):\n","    def tenf(koa):\n","        koa = tf.convert_to_tensor(koa, dtype = tf.float32)\n","        return koa\n","    def subuout(ain, biin, tau_in):\n","        # ain batchsize * 10, bin too Cheaked at January 26th\n","        uf = biin\n","        row, column = np.nonzero(ain)\n","        biin = biin.numpy()\n","        cin = tf.transpose(tf.ones((10, batch_size)) * tf.convert_to_tensor(biin[row, column], dtype = tf.float32))\n","        uin2 = tf.minimum(biin, cin)\n","        condition1 = tf.sqrt(tf.convert_to_tensor(tau_in, dtype = tf.float32)) * tf.linalg.norm(biin - uin2, ord=2, axis=1)  # axis 1 represents row\n","        T1 = (condition1 < 2)\n","        uf = uf.numpy()\n","        uf[T1,:] = uin2.numpy()[T1,:]\n","        # uf = uin2.numpy()\n","        return tf.convert_to_tensor(uf.T, dtype = tf.float32)  # tf.transpose(uf)  #\n","\n","    def subuin(ain, biin, uin, tau_in):\n","        epi = 3.e-7 \n","        u_p = biin.numpy()\n","        flag1 = u_p*0\n","        # kkk = np.zeros((10,2))\n","        # kkk[:,0] = ain.numpy()[0:10,0]\n","        # kkk[:,1] = biin.numpy()[0:10,0] \n","        # print(kkk, 1 - tau_in * biin[0:10,0] * biin[0:10,0] / 2, 1 + tau_in * biin[0:10,0] * biin[0:10,0] / 2)\n","        condition1 = (biin > 0) & (ain < 0.5) & (ain <= (1 - tau_in * biin * biin) / 2)\n","        condition2 = (biin < 0) & (ain > 0.5) & (ain >= (1 + tau_in * biin * biin) / 2)\n","        flag1[condition1] = 1\n","        flag1[condition2] = 1  \n","        if sum(sum(flag1))==0:\n","          flag2 = 0\n","        else:\n","          flag2 = 1\n","          u_p[condition1] = 0\n","          u_p[condition2] = epi\n","          uin  = uin +  1 * (tf.convert_to_tensor(u_p, dtype = tf.float32) - uin)\n","        return uin, flag2\n","\n","    def subw(win, bin, ain, tau_in, lr_in, gam_in, epoc, layer):\n","        # Win.shape n_{layer-1}, n_layer\n","        # A.shape n_{layer-1}, N\n","        # B.shape n_{layer}, N\n","        b = 2 * tau_in * tf.matmul(ain, tf.transpose(bin))\n","        A = 2 * tau_in * tf.matmul(ain, tf.transpose(ain)) + gam_in * tf.eye(len(b))\n","        d_W = tf.matmul(A, win) - b  # * gamma~~~~~~~~~~~~ \n","        if layer > 1:\n","          if epoc < -2:\n","            win -= lr_in *  d_W\n","            idx_T = np.nonzero(tf.ones((win.shape[0], 1)))[0]\n","          else:\n","            if layer==2:\n","              win -= 0.01 * lr_in *  d_W\n","              idx_T = np.nonzero(tf.ones((win.shape[0], 1)))[0]\n","            else:\n","              W_try = win - lr_in *  d_W\n","              W_norm2 = prox_l0(W_try, sp0, vec_yn=0)\n","              idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","              idx_Tbar = np.argwhere(W_norm2 == 0)[:, 0]\n","              win = win.numpy()\n","              ain = ain.numpy()\n","              b2 =  2 * tau_in * tf.matmul(tf.matmul(ain[idx_T], tf.transpose(ain[idx_Tbar])), win[idx_Tbar])\n","              b = tenf(b.numpy()[idx_T]) - b2\n","              # if epoc > 20:\n","              #   win[idx_Tbar] -= 1.e-2 * win[idx_Tbar]\n","                # if epoc > 30:\n","                #     win[idx_Tbar] = 0\n","              A1 = tf.linalg.inv(2 * tau_in * tf.matmul(tenf(ain[idx_T]), tenf(tf.transpose(ain[idx_T]))) + gam_in * tf.eye(len(idx_T)))\n","              win[idx_T] = tf.matmul(A1, b)\n","        else:\n","          if epoc % 6 < -10:\n","            win -= lr_in *  d_W\n","          else:\n","            win = tf.matmul(tf.linalg.inv(A), b)\n","          idx_T = np.nonzero(tf.ones((win.shape[0], 1)))[0]\n","        return tf.convert_to_tensor(win, dtype = tf.float32), idx_T  # tf.convert_to_tensor(, dtype = tf.float32)\n","\n","    def subv(vin, idx, ain, bin, cin, pa_in, tau_in, layer):  # 显示解\n","        # print(len(idx), vin.shape)\n","        # vin: 1000x64;  ain:  1000[idx+1]x10;  bin:  10, 64;   cin: 1000[idx+1], 64,\n","        # for j in range(2):\n","        print(tau_in, pa_in)\n","        b = 2 * tau_in * tf.matmul(ain, bin) + 1000 * 2 * pa_in * cin\n","        A1 = tf.linalg.inv(2 * tau_in * tf.matmul(ain, tf.transpose(ain)) +  1000 * 2 * pa_in * tf.eye(len(b)))\n","        vin = tf.matmul(A1, b)\n","        return vin  # tf.convert_to_tensor(vin, dtype = tf.float32)\n","\n","    for layer in range(3, 0, -1):\n","        if layer == 3:\n","            # Y 10*64;  W3 (1000, 10); V3 (10, 64);\n","            cost1be = pa[3] * tf.square( tf.norm(Y - hardmax(pinter[\"U3\"])) ) + tau[3] * tf.square(tf.norm( pinter[\"U3\"] - tf.matmul(tf.transpose(parameters['W3']),  pinter[\"V2\"]))  )\n","            pinter['U' + str(layer)]= \\\n","                subuout(tf.transpose(Y), tf.transpose(tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)])), tau[layer] * pa[layer])\n","\n","            cost1af = pa[3] * tf.square( tf.norm(Y - hardmax(pinter[\"U3\"])) ) + tau[3] * tf.square(tf.norm( pinter[\"U3\"] - tf.matmul(tf.transpose(parameters['W3']),  pinter[\"V2\"])))\n","            print(\"U3\", cost1be.numpy(), cost1af.numpy())\n","            cost1be = tau[3] * tf.square(tf.norm( pinter[\"U3\"] - tf.matmul(tf.transpose(parameters['W3']),  pinter[\"V2\"]))) + gam/2 * (tf.square( tf.norm(parameters['W3'])))\n","            parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], lr, gam, epoc, layer)\n","            cost2af = tau[3] * tf.square(tf.norm( pinter[\"U3\"] - tf.matmul(tf.transpose(parameters['W3']),  pinter[\"V2\"]))) + gam/2 * (tf.square( tf.norm(parameters['W3'])))\n","            print(\"W3\",cost1be.numpy(), cost2af.numpy())\n","\n","        else:\n","            costvbe = tau[layer+1] * tf.square(tf.norm(pinter['U' + str(layer + 1)] - tf.matmul(tf.transpose(parameters[\"W\" + str(layer + 1)]), pinter['V' + str(layer)]))) + pa[layer] * tf.square(tf.norm(pinter['V' + str(layer)] - sgn(pinter['U' + str(layer)])))\n","            print((tau[layer+1] * tf.square(tf.norm(pinter['U' + str(layer + 1)] - tf.matmul(tf.transpose(parameters[\"W\" + str(layer + 1)]), pinter['V' + str(layer)])))).numpy(),\n","                  (pa[layer] * tf.square(tf.norm(pinter['V' + str(layer)] - sgn(pinter['U' + str(layer)])))).numpy())\n","            pinter['V' + str(layer)] = subv(pinter['V' + str(layer)], parameters[\"idx\" + str(layer + 1)],\n","                                            parameters[\"W\" + str(layer + 1)], pinter['U' + str(layer + 1)],\n","                                            sgn(pinter['U' + str(layer)]), pa[layer], tau[layer+1], layer)  # , parameters[\"idx\" + str(layer + 1)])\n","            costvaf = tau[layer+1] * tf.square(tf.norm(pinter['U' + str(layer + 1)] - tf.matmul(tf.transpose(parameters[\"W\" + str(layer + 1)]), pinter['V' + str(layer)]))) + pa[layer] * tf.square(tf.norm(pinter['V' + str(layer)] - sgn(pinter['U' + str(layer)])))\n","            \n","            print((tau[layer+1] * tf.square(tf.norm(pinter['U' + str(layer + 1)] - tf.matmul(tf.transpose(parameters[\"W\" + str(layer + 1)]), pinter['V' + str(layer)])))).numpy(),\n","                   (pa[layer] * tf.square(tf.norm(pinter['V' + str(layer)] - sgn(pinter['U' + str(layer)])))).numpy())\n","            \n","            print(\"V\"+ str(layer), costvbe.numpy(), costvaf.numpy())\n","\n","            costubf = pa[layer] * tf.square( tf.norm(pinter['V' + str(layer)] - sgn(pinter['U' + str(layer)]))) + tau[layer] * tf.square(tf.norm( pinter['U' + str(layer)] - tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]),  pinter['V' + str(layer - 1)])))\n","            pinter['U' + str(layer)], flag2 = subuin(pinter['V' + str(layer)],\n","                                              tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)]),\n","                                              pinter['U' + str(layer)],  tau[layer] / pa[layer])\n","            costuaf = pa[layer] * tf.square( tf.norm(pinter['V' + str(layer)] - sgn(pinter['U' + str(layer)]))) + tau[layer] * tf.square(tf.norm( pinter['U' + str(layer)] - tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]),  pinter['V' + str(layer - 1)])))\n","            print(\"Uin\"+ str(layer), costubf.numpy(), costuaf.numpy())\n","\n","            if flag2:\n","              costwinbf = tau[layer] * tf.square(tf.norm( pinter['U' + str(layer)] - tf.matmul(tf.transpose(parameters['W' + str(layer)]),   pinter['V' + str(layer - 1)]))) + gam/2 * (tf.square( tf.norm(parameters['W' + str(layer)])))\n","              parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                  subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], lr,  gam, epoc, layer)\n","              costwinaf = tau[layer] * tf.square(tf.norm( pinter['U' + str(layer)] - tf.matmul(tf.transpose(parameters['W' + str(layer)]),   pinter['V' + str(layer - 1)]))) + gam/2 * (tf.square( tf.norm(parameters['W' + str(layer)])))\n","              print(\"Win\"+ str(layer), costwinbf.numpy(), costwinaf.numpy())\n","\n","            # cost3 = pa[2] * tf.square( tf.norm(C[\"V2\"] - sgn(C[\"U2\"])) ) + tau[2] * tf.square( tf.norm(C[\"U2\"] - tf.matmul(tf.transpose(P['W2']), C[\"V1\"])) )\n","            # print(\"Cheack out, now-------------------\")\n","            # print(pa[2], tf.square( tf.norm(C[\"V2\"] - sgn(C[\"U2\"])) ), tau[2], tf.square( tf.norm(C[\"U2\"] - tf.matmul(tf.transpose(P['W2']), C[\"V1\"])) ))\n","            # print(\"------------------------------------\")\n","            # cost4 =  pa[1] * tf.square( tf.norm(C[\"V1\"] - sgn(C[\"U1\"])) ) + tau[1] * tf.square( tf.norm(C[\"U1\"] - tf.matmul(tf.transpose(P['W1']), X)) )\n","            # cost5 = gam * (tf.square( tf.norm(P['W3'])) + tf.square(tf.norm(P['W2'])) + tf.square(tf.norm(P['W1'])))\n","            \n","    return parameters, pinter"]},{"cell_type":"markdown","metadata":{"id":"pDMKOIPsYL_q"},"source":["# **3 Train 0/1 DNN by PBCD**"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9hLbeR8tlOv","outputId":"84cb1b29-ec8a-40b6-a22e-e25113015988","executionInfo":{"status":"ok","timestamp":1652903703790,"user_tz":-60,"elapsed":5718652,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------\n","U3 [99.200005] [73.5498]\n","W3 [11.533136] [9.27671]\n","[9.276692] [0.]\n","[0.0001] [0.0001]\n","[9.185942] [4.5247652e-05]\n","V2 [9.276692] [9.1859865]\n","Uin2 [4.5247652e-05] [4.5247652e-05]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[3.552563e-10] [1.6776636e-11]\n","V1 [0.] [3.7203293e-10]\n","Uin1 [1.6776636e-11] [1.6776636e-11]\n","Cheack out, now-------------------\n","[0.0001] 0.45247653 [0.0001] 3.552563e-06\n","------------------------------------\n","After [62.016674] [9.185942] [4.524801e-05] [1.6776636e-11] 0.0033593269 [71.20602]\n","Famal train: The :0-th epoch, current super1:7.2. and the current accuracy in this step: 8926.0\n","Updating best accuracy in this epoch:8926.0\n","-------------------------------------------\n","U3 [71.202614] [66.99572]\n","W3 [7.945735] [5.5809903]\n","[4.819643] [4.5247652e-05]\n","[0.0001] [0.0001]\n","[0.0309345] [0.00010619]\n","V2 [4.8196883] [0.0310407]\n","Uin2 [0.00010619] [0.00010619]\n","Win2 [0.00114483] [0.00114483]\n","[2.5673313e-14] [1.6776636e-11]\n","[0.0001] [1.e-05]\n","[3.723578e-11] [1.7252932e-11]\n","V1 [1.680231e-11] [5.4488712e-11]\n","Uin1 [1.7252932e-11] [1.7252932e-11]\n","Cheack out, now-------------------\n","[0.0001] 1.0619395 [0.0001] 3.7235782e-07\n","------------------------------------\n","After [59.050003] [0.0309345] [0.00010619] [1.7252932e-11] 1.5260193 [60.607063]\n","Famal train: The :1-th epoch, current super1:7.2. and the current accuracy in this step: 8954.0\n","Updating best accuracy in this epoch:8954.0\n","-------------------------------------------\n","U3 [92.3] [63.03583]\n","W3 [6.8138404] [5.2474666]\n","[5.2469044] [0.]\n","[0.0001] [0.0001]\n","[3.8273695] [0.00055263]\n","V2 [5.2469044] [3.827922]\n","Uin2 [0.00055263] [0.00055263]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[3.5525163e-10] [1.677659e-11]\n","V1 [0.] [3.720282e-10]\n","Uin1 [1.677659e-11] [1.677659e-11]\n","Cheack out, now-------------------\n","[0.0001] 5.5262585 [0.0001] 3.5525163e-06\n","------------------------------------\n","After [56.983337] [3.8273695] [0.00055263] [1.677659e-11] 0.004449111 [60.81571]\n","Famal train: The :2-th epoch, current super1:7.2. and the current accuracy in this step: 8962.0\n","Updating best accuracy in this epoch:8962.0\n","-------------------------------------------\n","U3 [60.810707] [59.0957]\n","W3 [3.7129247] [2.4846804]\n","[2.4646237] [0.00055263]\n","[0.0001] [0.0001]\n","[0.5337237] [0.00072665]\n","V2 [2.4651763] [0.53445035]\n","Uin2 [0.00072665] [0.00072665]\n","Win2 [0.00114483] [0.00114483]\n","[2.6465043e-14] [1.677659e-11]\n","[0.0001] [1.e-05]\n","[3.7231548e-11] [1.7252774e-11]\n","V1 [1.6803055e-11] [5.448432e-11]\n","Uin1 [1.7252774e-11] [1.7252774e-11]\n","Cheack out, now-------------------\n","[0.0001] 7.2665296 [0.0001] 3.7231547e-07\n","------------------------------------\n","After [55.383335] [0.5337237] [0.00072665] [1.7252774e-11] 0.04343837 [55.96122]\n","Famal train: The :3-th epoch, current super1:7.2. and the current accuracy in this step: 8953.0\n","-------------------------------------------\n","Cheack out, now-------------------\n","[0.0001] 7.2665296 [0.0001] 14479510.0\n","------------------------------------\n","After [55.383335] [94.52118] [1447.9517] [81.56766] 0.043651477 [1679.4675]\n","Famal train: The :4-th epoch, current super1:7.2. and the current accuracy in this step: 8890.0\n","-------------------------------------------\n","U3 [149.90451] [36.221424]\n","W3 [1.1748093] [0.2805019]\n","[0.2172729] [0.00072665]\n","[0.0001] [0.0001]\n","[0.06635158] [0.00052427]\n","V2 [0.21799955] [0.06687585]\n","Uin2 [1447.9514] [58.783722]\n","Win2 [50.549824] [50.47581]\n","[50.47459] [1.7252774e-11]\n","[0.0001] [1.e-05]\n","[49.03011] [0.00069498]\n","V1 [50.47459] [49.030804]\n","Uin1 [81.56835] [4.7129784]\n","Win1 [4.1995893] [1.5652032]\n","Cheack out, now-------------------\n","[0.0001] 82351.2 [0.0001] 490301.1\n","------------------------------------\n","After [35.066673] [0.06635158] [57.26523] [2.0785265] 0.13012542 [94.60691]\n","Famal train: The :5-th epoch, current super1:7.2. and the current accuracy in this step: 8907.0\n","-------------------------------------------\n","U3 [90.18333] [65.09714]\n","W3 [14.893691] [12.231432]\n","[12.2270975] [0.]\n","[0.0001] [0.0001]\n","[7.976759] [0.00113755]\n","V2 [12.2270975] [7.9778967]\n","Uin2 [0.00113755] [0.00113755]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[3.3504205e-10] [1.5025333e-11]\n","V1 [0.] [3.500674e-10]\n","Uin1 [1.5025333e-11] [1.5025333e-11]\n","Cheack out, now-------------------\n","[0.0001] 11.375533 [0.0001] 3.3504207e-06\n","------------------------------------\n","After [50.266674] [7.976759] [0.00113755] [1.5025333e-11] 0.012337163 [58.256905]\n","Famal train: The :6-th epoch, current super1:7.2. and the current accuracy in this step: 8936.0\n","-------------------------------------------\n","U3 [58.24343] [56.856327]\n","W3 [7.6606536] [4.3616295]\n","[4.3495197] [0.00113755]\n","[0.0001] [0.0001]\n","[2.3370492] [0.00175784]\n","V2 [4.3506575] [2.338807]\n","Uin2 [0.00175784] [0.00175784]\n","Win2 [0.00121891] [0.00121891]\n","[2.174182e-14] [1.5025333e-11]\n","[0.0001] [1.e-05]\n","[4.155115e-11] [1.5449942e-11]\n","V1 [1.5047075e-11] [5.700109e-11]\n","Uin1 [1.5449942e-11] [1.5449942e-11]\n","Cheack out, now-------------------\n","[0.0001] 17.578432 [0.0001] 4.1551152e-07\n","------------------------------------\n","After [49.20001] [2.3370492] [0.00175784] [1.5449942e-11] 0.02788684 [51.5667]\n","Famal train: The :7-th epoch, current super1:7.2. and the current accuracy in this step: 8959.0\n","-------------------------------------------\n","U3 [87.03334] [58.238697]\n","W3 [9.167467] [7.775026]\n","[7.7641068] [0.]\n","[0.0001] [0.0001]\n","[2.5380938] [0.00121725]\n","V2 [7.7641068] [2.539311]\n","Uin2 [0.00121725] [0.00121725]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[3.3503367e-10] [1.5025394e-11]\n","V1 [0.] [3.5005907e-10]\n","Uin1 [1.5025394e-11] [1.5025394e-11]\n","Cheack out, now-------------------\n","[0.0001] 12.17246 [0.0001] 3.3503368e-06\n","------------------------------------\n","After [49.08334] [2.5380938] [0.00121725] [1.5025394e-11] 0.025505442 [51.648155]\n","Famal train: The :8-th epoch, current super1:7.2. and the current accuracy in this step: 8971.0\n","Updating best accuracy in this epoch:8971.0\n","-------------------------------------------\n","U3 [51.621433] [50.702282]\n","W3 [2.5131998] [1.4770459]\n","[1.4590008] [0.00121725]\n","[0.0001] [0.0001]\n","[0.8995277] [0.0011861]\n","V2 [1.4602181] [0.9007138]\n","Uin2 [0.0011861] [0.0011861]\n","Win2 [0.00121891] [0.00121891]\n","[2.3470527e-14] [1.5025394e-11]\n","[0.0001] [1.e-05]\n","[4.1552012e-11] [1.5449872e-11]\n","V1 [1.5048865e-11] [5.7001882e-11]\n","Uin1 [1.5449872e-11] [1.5449872e-11]\n","Cheack out, now-------------------\n","[0.0001] 11.861023 [0.0001] 4.1552013e-07\n","------------------------------------\n","After [48.2] [0.8995277] [0.0011861] [1.5449872e-11] 0.039757617 [49.140472]\n","Famal train: The :9-th epoch, current super1:7.2. and the current accuracy in this step: 8980.0\n","Updating best accuracy in this epoch:8980.0\n","-------------------------------------------\n","Cheack out, now-------------------\n","[0.0001] 11.861023 [0.0001] 10532647.0\n","------------------------------------\n","After [48.2] [104.043106] [1053.2659] [67.26802] 0.039953206 [1272.817]\n","Famal train: The :10-th epoch, current super1:7.2. and the current accuracy in this step: 8997.0\n","Updating best accuracy in this epoch:8997.0\n","-------------------------------------------\n","U3 [152.2431] [49.09187]\n","W3 [1.5765752] [0.7247805]\n","[0.6960806] [0.0011861]\n","[0.0001] [0.0001]\n","[0.5084391] [0.00113819]\n","V2 [0.6972667] [0.50957733]\n","Uin2 [1053.2657] [37.632156]\n","Win2 [32.84881] [32.812344]\n","[32.811058] [1.5449872e-11]\n","[0.0001] [1.e-05]\n","[31.795315] [0.00047799]\n","V1 [32.811058] [31.795794]\n","Uin1 [67.2685] [3.3739798]\n","Win1 [3.2866528] [1.3951536]\n","Cheack out, now-------------------\n","[0.0001] 47846.344 [0.0001] 317953.16\n","------------------------------------\n","After [47.53334] [0.5084391] [36.57995] [1.4824271] 0.061367005 [86.16552]\n","Famal train: The :11-th epoch, current super1:7.2. and the current accuracy in this step: 8992.0\n","-------------------------------------------\n","U3 [81.33333] [61.536236]\n","W3 [16.631605] [13.881918]\n","[13.878496] [0.]\n","[0.0001] [0.0001]\n","[8.292818] [0.00137997]\n","V2 [13.878496] [8.294198]\n","Uin2 [0.00137997] [0.00137997]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.9423036e-10] [1.2543781e-11]\n","V1 [0.] [3.0677413e-10]\n","Uin1 [1.2543781e-11] [1.2543781e-11]\n","Cheack out, now-------------------\n","[0.0001] 13.799706 [0.0001] 2.9423036e-06\n","------------------------------------\n","After [44.93333] [8.292818] [0.00137997] [1.2543781e-11] 0.010810705 [53.238342]\n","Famal train: The :12-th epoch, current super1:7.2. and the current accuracy in this step: 9000.0\n","Updating best accuracy in this epoch:9000.0\n","-------------------------------------------\n","U3 [53.22615] [51.47268]\n","W3 [7.726093] [4.8959947]\n","[4.886456] [0.00137997]\n","[0.0001] [0.0001]\n","[2.4788113] [0.00223118]\n","V2 [4.887836] [2.4810424]\n","Uin2 [0.00223118] [0.00221339]\n","Win2 [0.00128843] [0.00128843]\n","[1.0917512e-06] [1.2543781e-11]\n","[0.0001] [1.e-05]\n","[1.069591e-06] [2.384207e-11]\n","V1 [1.0917637e-06] [1.0696149e-06]\n","Uin1 [2.384207e-11] [2.384207e-11]\n","Cheack out, now-------------------\n","[0.0001] 22.122995 [0.0001] 0.01069591\n","------------------------------------\n","After [43.750008] [2.4788113] [0.00221337] [2.384207e-11] 0.023044355 [46.254078]\n","Famal train: The :13-th epoch, current super1:7.2. and the current accuracy in this step: 9010.0\n","Updating best accuracy in this epoch:9010.0\n","-------------------------------------------\n","U3 [79.4] [55.407177]\n","W3 [10.966711] [9.350527]\n","[9.343083] [0.]\n","[0.0001] [0.0001]\n","[3.15875] [0.00144371]\n","V2 [9.343083] [3.1601937]\n","Uin2 [0.00144371] [0.00144371]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.942344e-10] [1.2543711e-11]\n","V1 [0.] [3.067781e-10]\n","Uin1 [1.2543711e-11] [1.2543711e-11]\n","Cheack out, now-------------------\n","[0.0001] 14.437096 [0.0001] 2.942344e-06\n","------------------------------------\n","After [44.450005] [3.15875] [0.00144371] [1.2543711e-11] 0.018854765 [47.62905]\n","Famal train: The :14-th epoch, current super1:7.2. and the current accuracy in this step: 9016.0\n","Updating best accuracy in this epoch:9016.0\n","-------------------------------------------\n","Cheack out, now-------------------\n","[0.0001] 14.437096 [0.0001] 6634316.0\n","------------------------------------\n","After [44.450005] [82.30542] [663.43304] [62.14102] 0.019045169 [852.3485]\n","Famal train: The :15-th epoch, current super1:7.2. and the current accuracy in this step: 9009.0\n","-------------------------------------------\n","U3 [84.48334] [70.4855]\n","W3 [18.159607] [15.376424]\n","[15.376306] [0.]\n","[0.0001] [0.0001]\n","[14.3530445] [0.00049682]\n","V2 [15.376306] [14.353541]\n","Uin2 [0.00049682] [0.00049682]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.863389e-10] [1.1061338e-11]\n","V1 [0.] [2.9740024e-10]\n","Uin1 [1.1061338e-11] [1.1061338e-11]\n","Cheack out, now-------------------\n","[0.0001] 4.968178 [0.0001] 2.863389e-06\n","------------------------------------\n","After [52.333336] [14.3530445] [0.00049682] [1.1061338e-11] 0.004389578 [66.69126]\n","Famal train: The :16-th epoch, current super1:7.2. and the current accuracy in this step: 9007.0\n","-------------------------------------------\n","U3 [66.68638] [63.18354]\n","W3 [12.966993] [9.544958]\n","[9.540715] [0.00049682]\n","[0.0001] [0.0001]\n","[6.5497227] [0.00150397]\n","V2 [9.541212] [6.5512266]\n","Uin2 [0.00150397] [0.00156618]\n","Win2 [0.00135758] [0.00135758]\n","[3.5454555e-06] [1.1061338e-11]\n","[0.0001] [1.e-05]\n","[3.4641705e-06] [5.137908e-11]\n","V1 [3.5454666e-06] [3.464222e-06]\n","Uin1 [5.137908e-11] [5.137908e-11]\n","Cheack out, now-------------------\n","[0.0001] 15.626353 [0.0001] 0.034641705\n","------------------------------------\n","After [50.216663] [6.5497227] [0.0015661] [5.137908e-11] 0.012640148 [56.780598]\n","Famal train: The :17-th epoch, current super1:7.2. and the current accuracy in this step: 9014.0\n","-------------------------------------------\n","U3 [80.833336] [59.77523]\n","W3 [11.029471] [9.70405]\n","[9.700127] [0.]\n","[0.0001] [0.0001]\n","[5.368914] [0.00112621]\n","V2 [9.700127] [5.3700404]\n","Uin2 [0.00112621] [0.00112621]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.8632766e-10] [1.1061519e-11]\n","V1 [0.] [2.9738917e-10]\n","Uin1 [1.1061519e-11] [1.1061519e-11]\n","Cheack out, now-------------------\n","[0.0001] 11.26207 [0.0001] 2.8632767e-06\n","------------------------------------\n","After [48.75] [5.368914] [0.00112621] [1.1061519e-11] 0.012000246 [54.132042]\n","Famal train: The :18-th epoch, current super1:7.2. and the current accuracy in this step: 9025.0\n","Updating best accuracy in this epoch:9025.0\n","-------------------------------------------\n","U3 [54.118916] [51.334637]\n","W3 [5.4218893] [3.2749753]\n","[3.265058] [0.00112621]\n","[0.0001] [0.0001]\n","[2.0465598] [0.00148594]\n","V2 [3.2661843] [2.0480456]\n","Uin2 [0.00148594] [0.00157687]\n","Win2 [0.00135403] [0.00135403]\n","[2.1421982e-09] [1.1061519e-11]\n","[0.0001] [1.e-05]\n","[2.1235866e-09] [1.1447902e-11]\n","V1 [2.1532598e-09] [2.1350346e-09]\n","Uin1 [1.1447902e-11] [1.1447902e-11]\n","Cheack out, now-------------------\n","[0.0001] 15.768645 [0.0001] 2.1235866e-05\n","------------------------------------\n","After [45.91667] [2.0465598] [0.00157687] [1.1447902e-11] 0.023988176 [47.988792]\n","Famal train: The :19-th epoch, current super1:7.2. and the current accuracy in this step: 9023.0\n","-------------------------------------------\n","Cheack out, now-------------------\n","[0.0001] 15.768645 [0.0001] 5606954.5\n","------------------------------------\n","After [45.91667] [102.18875] [560.697] [54.07835] 0.024171902 [762.90497]\n","Famal train: The :20-th epoch, current super1:7.2. and the current accuracy in this step: 8977.0\n","-------------------------------------------\n","U3 [148.10542] [47.85846]\n","W3 [2.9350488] [1.3846357]\n","[1.3651264] [0.00157686]\n","[0.0001] [0.0001]\n","[1.1182895] [0.00138828]\n","V2 [1.3667033] [1.1196778]\n","Uin2 [560.69684] [18.661705]\n","Win2 [18.22395] [18.212206]\n","[18.210787] [1.1447902e-11]\n","[0.0001] [1.e-05]\n","[17.636362] [0.00026922]\n","V1 [18.210787] [17.636631]\n","Uin1 [54.07862] [2.162087]\n","Win1 [2.0691774] [0.92108524]\n","Cheack out, now-------------------\n","[0.0001] 4391.7417 [0.0001] 176363.62\n","------------------------------------\n","After [44.93333] [1.1182895] [18.075537] [1.0139602] 0.043420948 [65.18454]\n","Famal train: The :21-th epoch, current super1:7.2. and the current accuracy in this step: 8996.0\n","-------------------------------------------\n","U3 [76.933334] [68.26631]\n","W3 [19.119154] [15.937512]\n","[15.923117] [0.]\n","[0.0001] [0.0001]\n","[6.278311] [0.0012441]\n","V2 [15.923117] [6.279555]\n","Uin2 [0.0012441] [0.0012441]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[3.2175426e-10] [1.0137536e-11]\n","V1 [0.] [3.318918e-10]\n","Uin1 [1.0137536e-11] [1.0137536e-11]\n","Cheack out, now-------------------\n","[0.0001] 12.440975 [0.0001] 3.2175428e-06\n","------------------------------------\n","After [49.166664] [6.278311] [0.0012441] [1.0137536e-11] 0.033193756 [55.479416]\n","Famal train: The :22-th epoch, current super1:7.2. and the current accuracy in this step: 9000.0\n","-------------------------------------------\n","U3 [55.444977] [52.057236]\n","W3 [5.571633] [4.2688284]\n","[4.249493] [0.0012441]\n","[0.0001] [0.0001]\n","[2.4383554] [0.00176556]\n","V2 [4.250737] [2.440121]\n","Uin2 [0.00176556] [0.00175559]\n","Win2 [0.00141833] [0.00141833]\n","[6.148754e-08] [1.0137536e-11]\n","[0.0001] [1.e-05]\n","[6.0125494e-08] [1.1233031e-11]\n","V1 [6.149768e-08] [6.013673e-08]\n","Uin1 [1.1233031e-11] [1.1233031e-11]\n","Cheack out, now-------------------\n","[0.0001] 17.55524 [0.0001] 0.0006012549\n","------------------------------------\n","After [46.5] [2.4383554] [0.00175558] [1.1233031e-11] 0.043073025 [48.98318]\n","Famal train: The :23-th epoch, current super1:7.2. and the current accuracy in this step: 9002.0\n","-------------------------------------------\n","U3 [74.65] [59.665665]\n","W3 [13.251668] [11.656098]\n","[11.63697] [0.]\n","[0.0001] [0.0001]\n","[2.7109544] [0.00132217]\n","V2 [11.63697] [2.7122767]\n","Uin2 [0.00132217] [0.00132217]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[3.2175254e-10] [1.0137554e-11]\n","V1 [0.] [3.318901e-10]\n","Uin1 [1.0137554e-11] [1.0137554e-11]\n","Cheack out, now-------------------\n","[0.0001] 13.221747 [0.0001] 3.2175255e-06\n","------------------------------------\n","After [46.43333] [2.7109544] [0.00132218] [1.0137554e-11] 0.042659797 [49.188267]\n","Famal train: The :24-th epoch, current super1:7.2. and the current accuracy in this step: 9003.0\n","-------------------------------------------\n","Cheack out, now-------------------\n","[0.0001] 13.221747 [0.0001] 3913678.2\n","------------------------------------\n","After [46.43333] [82.67841] [391.36914] [46.53424] 0.04284767 [567.058]\n","Famal train: The :25-th epoch, current super1:7.2. and the current accuracy in this step: 9025.0\n","-------------------------------------------\n","U3 [78.01667] [74.21286]\n","W3 [21.91532] [18.73092]\n","[18.719088] [0.]\n","[0.0001] [0.0001]\n","[8.212265] [0.00177795]\n","V2 [18.719088] [8.214043]\n","Uin2 [0.00177795] [0.00177795]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.7340005e-10] [9.130677e-12]\n","V1 [0.] [2.8253072e-10]\n","Uin1 [9.130677e-12] [9.130677e-12]\n","Cheack out, now-------------------\n","[0.0001] 17.779486 [0.0001] 2.7340006e-06\n","------------------------------------\n","After [52.316673] [8.212265] [0.00177795] [9.130677e-12] 0.028248848 [60.558964]\n","Famal train: The :26-th epoch, current super1:7.2. and the current accuracy in this step: 9024.0\n","-------------------------------------------\n","U3 [60.52894] [57.962936]\n","W3 [6.874763] [5.1047482]\n","[5.08747] [0.00177795]\n","[0.0001] [0.0001]\n","[2.9834132] [0.00209627]\n","V2 [5.089248] [2.9855094]\n","Uin2 [0.00209627] [0.0021901]\n","Win2 [0.00148342] [0.00148342]\n","[6.0852345e-09] [9.130677e-12]\n","[0.0001] [1.e-05]\n","[5.9801573e-09] [9.546549e-12]\n","V1 [6.094365e-09] [5.989704e-09]\n","Uin1 [9.546549e-12] [9.546549e-12]\n","Cheack out, now-------------------\n","[0.0001] 21.90098 [0.0001] 5.9801572e-05\n","------------------------------------\n","After [51.100006] [2.9834132] [0.0021901] [9.546549e-12] 0.039140094 [54.12475]\n","Famal train: The :27-th epoch, current super1:7.2. and the current accuracy in this step: 9038.0\n","Updating best accuracy in this epoch:9038.0\n","-------------------------------------------\n","U3 [75.933334] [65.591896]\n","W3 [15.325838] [13.419821]\n","[13.403522] [0.]\n","[0.0001] [0.0001]\n","[3.6666405] [0.00172735]\n","V2 [13.403522] [3.6683679]\n","Uin2 [0.00172735] [0.00172735]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.734021e-10] [9.130797e-12]\n","V1 [0.] [2.825329e-10]\n","Uin1 [9.130797e-12] [9.130797e-12]\n","Cheack out, now-------------------\n","[0.0001] 17.273462 [0.0001] 2.734021e-06\n","------------------------------------\n","After [50.283337] [3.6666405] [0.00172735] [9.130797e-12] 0.037181508 [53.988888]\n","Famal train: The :28-th epoch, current super1:7.2. and the current accuracy in this step: 9037.0\n","-------------------------------------------\n","U3 [53.949978] [52.570766]\n","W3 [3.520393] [1.8969165]\n","[1.8735781] [0.00172735]\n","[0.0001] [0.0001]\n","[1.0826871] [0.00165914]\n","V2 [1.8753054] [1.0843463]\n","Uin2 [0.00165914] [0.00175914]\n","Win2 [0.00148341] [0.00148341]\n","[5.6722984e-15] [9.130797e-12]\n","[0.0001] [1.e-05]\n","[4.4024735e-11] [9.473432e-12]\n","V1 [9.136469e-12] [5.3498168e-11]\n","Uin1 [9.473432e-12] [9.473432e-12]\n","Cheack out, now-------------------\n","[0.0001] 17.591446 [0.0001] 4.4024736e-07\n","------------------------------------\n","After [49.066673] [1.0826871] [0.00175914] [9.473432e-12] 0.051260684 [50.20238]\n","Famal train: The :29-th epoch, current super1:7.2. and the current accuracy in this step: 9043.0\n","Updating best accuracy in this epoch:9043.0\n","-------------------------------------------\n","Cheack out, now-------------------\n","[0.0001] 17.591446 [0.0001] 3199477.8\n","------------------------------------\n","After [49.066673] [72.68391] [319.94952] [40.40265] 0.051442012 [482.1542]\n","Famal train: The :30-th epoch, current super1:7.2. and the current accuracy in this step: 8970.0\n","-------------------------------------------\n","U3 [121.75058] [49.520412]\n","W3 [1.7104168] [0.84388024]\n","[0.8115186] [0.00175914]\n","[0.0001] [0.0001]\n","[0.5951202] [0.00159738]\n","V2 [0.8132778] [0.5967176]\n","Uin2 [319.94934] [9.554712]\n","Win2 [9.472362] [9.468173]\n","[9.466626] [9.473432e-12]\n","[0.0001] [1.e-05]\n","[9.168911] [0.00014027]\n","V1 [9.466626] [9.169051]\n","Uin1 [40.40279] [1.3618468]\n","Win1 [1.3230455] [0.64013374]\n","Cheack out, now-------------------\n","[0.0001] 838.9719 [0.0001] 91689.11\n","------------------------------------\n","After [47.833336] [0.5951202] [9.252809] [0.6789132] 0.06952641 [58.429703]\n","Famal train: The :31-th epoch, current super1:7.2. and the current accuracy in this step: 8987.0\n","-------------------------------------------\n","U3 [79.41667] [89.04875]\n","W3 [29.59778] [24.689949]\n","[24.673536] [0.]\n","[0.0001] [0.0001]\n","[10.499937] [0.00220654]\n","V2 [24.673536] [10.502144]\n","Uin2 [0.00220654] [0.00220654]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.679702e-10] [8.354993e-12]\n","V1 [0.] [2.7632518e-10]\n","Uin1 [8.354993e-12] [8.354993e-12]\n","Cheack out, now-------------------\n","[0.0001] 22.06537 [0.0001] 2.6797022e-06\n","------------------------------------\n","After [59.483337] [10.499937] [0.00220654] [8.354993e-12] 0.037630387 [70.02311]\n","Famal train: The :32-th epoch, current super1:7.2. and the current accuracy in this step: 9004.0\n","-------------------------------------------\n","U3 [69.98328] [61.64296]\n","W3 [7.0760403] [4.296952]\n","[4.274513] [0.00220654]\n","[0.0001] [0.0001]\n","[1.7532959] [0.00248247]\n","V2 [4.276719] [1.7557783]\n","Uin2 [0.00248247] [0.00247132]\n","Win2 [0.00154697] [0.00154697]\n","[4.3773504e-08] [8.354993e-12]\n","[0.0001] [1.e-05]\n","[4.2565368e-08] [9.2861e-12]\n","V1 [4.378186e-08] [4.2574655e-08]\n","Uin1 [9.2861e-12] [9.2861e-12]\n","Cheack out, now-------------------\n","[0.0001] 24.712725 [0.0001] 0.0004256537\n","------------------------------------\n","After [54.583332] [1.7532959] [0.00247131] [9.2861e-12] 0.049681157 [56.388783]\n","Famal train: The :33-th epoch, current super1:7.2. and the current accuracy in this step: 9003.0\n","-------------------------------------------\n","U3 [74.41668] [72.59336]\n","W3 [19.599133] [16.930426]\n","[16.909548] [0.]\n","[0.0001] [0.0001]\n","[3.5611804] [0.00214052]\n","V2 [16.909548] [3.5633209]\n","Uin2 [0.00214052] [0.00214052]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.6797536e-10] [8.354969e-12]\n","V1 [0.] [2.7633032e-10]\n","Uin1 [8.354969e-12] [8.354969e-12]\n","Cheack out, now-------------------\n","[0.0001] 21.40516 [0.0001] 2.6797538e-06\n","------------------------------------\n","After [53.01667] [3.5611804] [0.00214052] [8.354969e-12] 0.04655983 [56.62655]\n","Famal train: The :34-th epoch, current super1:7.2. and the current accuracy in this step: 9011.0\n","-------------------------------------------\n","U3 [56.57785] [52.604042]\n","W3 [2.5249197] [1.522854]\n","[1.4975625] [0.00214052]\n","[0.0001] [0.0001]\n","[1.0353816] [0.00178676]\n","V2 [1.499703] [1.0371683]\n","Uin2 [0.00178676] [0.00178676]\n","Win2 [0.00154692] [0.00154692]\n","[4.6699764e-15] [8.354969e-12]\n","[0.0001] [1.e-05]\n","[4.63357e-11] [8.684904e-12]\n","V1 [8.3596385e-12] [5.5020603e-11]\n","Uin1 [8.684904e-12] [8.684904e-12]\n","Cheack out, now-------------------\n","[0.0001] 17.867643 [0.0001] 4.63357e-07\n","------------------------------------\n","After [50.100002] [1.0353816] [0.00178676] [8.684904e-12] 0.055386055 [51.192554]\n","Famal train: The :35-th epoch, current super1:7.2. and the current accuracy in this step: 9016.0\n","-------------------------------------------\n","Cheack out, now-------------------\n","[0.0001] 17.867643 [0.0001] 3059658.2\n","------------------------------------\n","After [50.100002] [82.09843] [305.96762] [37.90696] 0.05557022 [476.12857]\n","Famal train: The :36-th epoch, current super1:7.2. and the current accuracy in this step: 9006.0\n","-------------------------------------------\n","U3 [132.19843] [43.907692]\n","W3 [1.5663147] [1.0197524]\n","[0.9892254] [0.00178676]\n","[0.0001] [0.0001]\n","[0.7064858] [0.00174457]\n","V2 [0.99101216] [0.7082304]\n","Uin2 [305.96756] [9.351857]\n","Win2 [9.182823] [9.179056]\n","[9.177445] [8.684904e-12]\n","[0.0001] [1.e-05]\n","[8.879856] [0.00013925]\n","V1 [9.177445] [8.879995]\n","Uin1 [37.9071] [1.2687511]\n","Win1 [1.1978171] [0.60099375]\n","Cheack out, now-------------------\n","[0.0001] 1706.4434 [0.0001] 88798.56\n","------------------------------------\n","After [42.366673] [0.7064858] [9.050501] [0.6719091] 0.06607107 [52.861637]\n","Famal train: The :37-th epoch, current super1:7.2. and the current accuracy in this step: 9029.0\n","-------------------------------------------\n","U3 [68.76668] [67.77126]\n","W3 [21.118452] [18.333704]\n","[18.316875] [0.]\n","[0.0001] [0.0001]\n","[9.261566] [0.00130052]\n","V2 [18.316875] [9.262867]\n","Uin2 [0.00130052] [0.00130052]\n","[0.] [0.]\n","[0.0001] [1.e-05]\n","[2.5870298e-10] [7.661454e-12]\n","V1 [0.] [2.6636443e-10]\n","Uin1 [7.661454e-12] [7.661454e-12]\n","Cheack out, now-------------------\n","[0.0001] 13.005215 [0.0001] 2.58703e-06\n","------------------------------------\n","After [46.68334] [9.261566] [0.00130052] [7.661454e-12] 0.03867264 [55.98488]\n","Famal train: The :38-th epoch, current super1:7.2. and the current accuracy in this step: 9041.0\n","-------------------------------------------\n","U3 [55.944904] [52.62778]\n","W3 [7.827943] [6.1940036]\n","[6.1735] [0.00130052]\n","[0.0001] [0.0001]\n","[3.8800488] [0.00178299]\n","V2 [6.1748004] [3.8818316]\n","Uin2 [0.00178299] [0.00178299]\n","Win2 [0.00161115] [0.00161115]\n","[3.2915618e-15] [7.661454e-12]\n","[0.0001] [1.e-05]\n","[4.7861055e-11] [7.97318e-12]\n","V1 [7.664746e-12] [5.5834237e-11]\n","Uin1 [7.97318e-12] [7.97318e-12]\n","Cheack out, now-------------------\n","[0.0001] 17.829863 [0.0001] 4.7861056e-07\n","------------------------------------\n","After [44.816666] [3.8800488] [0.00178299] [7.97318e-12] 0.046024017 [48.744522]\n","Famal train: The :39-th epoch, current super1:7.2. and the current accuracy in this step: 9043.0\n","========================================================\n","=======================New accuracy=====================\n","The current best acc of all is: 9043.0, super parameter1 is: 7.2,\n","========================================================\n","The final best acc of all is: 9043.0, super parameter1 is: 7.2:\n"]}],"source":["label_train = label_trainall\n","label_test = label_testall\n","\n","batch_size = 60000\n","history1 = np.zeros((100,1))\n","history2 = np.zeros((100,1))\n","super1_all = [7.2]  # current best parameter 1.e-4:\n","epochs = 40\n","low_acc = -8000\n","best_acc_of_all = 0  # 1.2\n","best_para1 = 0.0072  # 0.3\n","for super1 in super1_all:\n","    sp0 = 1 - 0.45   # sparsity 0.7\n","    lr = 7.2e-4  # 6.e-2 ~ 6.e-1  turned at Feb 17th\n","    ship = 2.e-2  # slippage for the next step\n","    gam = 1.e-6  # 1.e-9  # turned at Feb 27th 6.e-2  --->  6.e-3   gamma for penalty norm2 \"W\"\n","    best_acc = 0\n","    parameters = load_parameters(os.path.join(directory, \"preparameter\"))\n","    # parameters, pa, tau, _ = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","\n","    pa = 1.e-5 * np.ones((4, 1))          # 每一层罚参数初始值\n","    pa[3] = 1 * 1000 / (2 * batch_size)   # 每一层罚参数初始值 8.33333333e-06 \n","    tau = 1.e-4 * np.ones((4, 1))         # 每一层罚参数初始值\n","    pa[2] = 1.e-4\n","\n","    # try:   \n","    for i in range(epochs):\n","        print(\"-------------------------------------------\")\n","        # aa25, _ = forward_propagation(trainall, parameters)\n","        # acc = (60000 - costloss(aa25, label_train))/6\n","        # history1[i] = acc\n","        \n","        if i ==4 or (i % 5 == 0 and i>8 and i<31) or (i % 6 == 0 and i>35): \n","          _, _, _, g_sum = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","          train_data_in = mini_batchs_generator(trainall.numpy().T, label_train.numpy().T, 256)\n","          for step, (img_train_in, label_train1_in) in enumerate(train_data_in):\n","              real_batch_size = len(label_train1_in)\n","              label_train_in = label_train1_in.T\n","              label_train_in = tf.convert_to_tensor(label_train_in, dtype=tf.float32)\n","              feat_train_in = tf.convert_to_tensor(img_train_in.T, dtype=tf.float32)  # batch_size * feat nums  -->  feat nums * batch_size\n","              # print(feat_train.shape, parameters[\"W1\"].shape)\n","              output, cache1 = forward_propagation_pre(feat_train_in, parameters)\n","              \n","              parameters, g_sum = optimizer_adam_l0(parameters, cache1, feat_train_in, label_train_in, g_sum,\n","                                                      epsilon, delta, sp0, rho_1, rho_2, step_ad)\n","        else:\n","          if i % 2 == 0:\n","            aa21, cache = forward_propagation(trainall, parameters)\n","          parameters, cache = optimizer(parameters, cache, trainall, label_train, i)\n","          \n","        if i % 1 == 0:\n","          merit = penaltyloss(parameters, cache, trainall, label_train)\n","          # acc_train = costloss(aa21, label_train)\n","          print(\"After\", merit[\"cost1\"], merit[\"cost2\"], merit[\"cost3\"], merit[\"cost4\"], \n","                merit[\"cost5\"], merit[\"costall\"])\n","        if merit[\"cost1\"] < 0.7 * merit[\"cost5\"]:\n","          gam *= 0.5\n","        # if i % 10 == 0 and i > 2:\n","        #   lr *= 0.8\n","\n","        aa25, _ = forward_propagation(testall, parameters)\n","        acc = 10000 - costloss(aa25, label_test)\n","        history2[i] = acc\n","\n","        if acc < low_acc or merit[\"costall\"] > 10000:\n","            print(\"Too low acc:{} or merit:{}.\".format(acc, merit[\"costall\"]))\n","            break\n","        else:\n","            print('Famal train: The :{}-th epoch, current super1:{}. and the current accuracy in this step: {}'.format(\n","                    i, super1, acc))\n","        if acc > best_acc:\n","            best_acc = acc\n","            print('Updating best accuracy in this epoch:{}'.format(best_acc))\n","\n","    if best_acc > best_acc_of_all:\n","        best_acc_of_all = best_acc\n","        best_para1 = super1\n","        print(\"========================================================\")\n","        print(\"=======================New accuracy=====================\")\n","        print(\"The current best acc of all is: {}, super parameter1 is: {},\".format(\n","            best_acc_of_all, best_para1))\n","        print(\"========================================================\")\n","    # except:\n","    #     print(\"There are some wrong things, but pass.\")\n","    #     continue   # break\n","print(\"The final best acc of all is: {}, super parameter1 is: {}:\".format(best_acc_of_all, best_para1))"]},{"cell_type":"code","source":["save_parameters(parameters, os.path.join(directory, \"finalparameter\"))"],"metadata":{"id":"T9xGe1DWRPwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wdfbrIcG6S7"},"outputs":[],"source":["np.save(os.path.join(direction, \"result/history1.npy\"), history1)\n","np.save(os.path.join(direction, \"result/history2.npy\"), history2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1650390025296,"user":{"displayName":"Jean Zhang","userId":"09922563489122533664"},"user_tz":-60},"id":"wvPh4H_djBaL","outputId":"f12e4b05-e43f-47d2-f32d-a2db01121b08"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[False False False]\n"," [False False False]] [[0. 0. 0.]\n"," [0. 0. 0.]]\n"]}],"source":["import tensorflow as tf\n","import torch as tc\n","import numpy as np\n","from tqdm import tqdm, trange\n","import time\n","\n","import time\n","from tqdm import tqdm, trange\n","\n","\n","a1 = tf.random.uniform([2,3], minval=-1, maxval=1, dtype=tf.dtypes.float32, seed=None, name=None)\n","a1 = a1.numpy()\n","a1[:,:] = 0\n","condition1 = (a1 < 0) \n","a1[condition1] = 999\n","print(condition1, a1)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Fashion_515.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}